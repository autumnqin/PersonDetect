I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: GeForce GTX 1080
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:82:00.0
Total memory: 7.92GiB
Free memory: 6.64GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:82:00.0)
batch_size:64, num_classes:1001
Tensor("tower_0/concat:0", shape=(64, 2), dtype=int32, device=/device:GPU:2)
2017-03-29 02:32:27.429531: step 0, loss = 7.51 (1.3 examples/sec; 50.328 sec/batch)
2017-03-29 02:32:59.135256: step 10, loss = 7.58 (127.0 examples/sec; 0.504 sec/batch)
2017-03-29 02:33:05.854423: step 20, loss = 7.53 (135.6 examples/sec; 0.472 sec/batch)
2017-03-29 02:33:12.173483: step 30, loss = 7.50 (34.4 examples/sec; 1.862 sec/batch)
2017-03-29 02:33:25.096733: step 40, loss = 7.50 (28.6 examples/sec; 2.237 sec/batch)
2017-03-29 02:33:33.388120: step 50, loss = 7.52 (150.9 examples/sec; 0.424 sec/batch)
2017-03-29 02:33:38.353111: step 60, loss = 7.49 (144.0 examples/sec; 0.445 sec/batch)
2017-03-29 02:33:45.265086: step 70, loss = 7.46 (134.2 examples/sec; 0.477 sec/batch)
2017-03-29 02:33:56.147090: step 80, loss = 7.50 (133.5 examples/sec; 0.479 sec/batch)
2017-03-29 02:34:01.518132: step 90, loss = 7.47 (153.9 examples/sec; 0.416 sec/batch)
2017-03-29 02:34:06.910743: step 100, loss = 7.49 (153.2 examples/sec; 0.418 sec/batch)
2017-03-29 02:34:15.420473: step 110, loss = 7.52 (104.2 examples/sec; 0.614 sec/batch)
2017-03-29 02:34:22.941204: step 120, loss = 7.46 (63.6 examples/sec; 1.006 sec/batch)
2017-03-29 02:34:30.563682: step 130, loss = 7.48 (150.1 examples/sec; 0.426 sec/batch)
2017-03-29 02:34:37.818410: step 140, loss = 7.49 (48.0 examples/sec; 1.334 sec/batch)
2017-03-29 02:34:45.344176: step 150, loss = 7.48 (42.6 examples/sec; 1.501 sec/batch)
2017-03-29 02:34:49.687457: step 160, loss = 7.47 (142.2 examples/sec; 0.450 sec/batch)
2017-03-29 02:34:55.052712: step 170, loss = 7.51 (53.7 examples/sec; 1.191 sec/batch)
2017-03-29 02:35:02.329343: step 180, loss = 7.50 (36.0 examples/sec; 1.778 sec/batch)
2017-03-29 02:35:09.617103: step 190, loss = 7.47 (40.1 examples/sec; 1.594 sec/batch)
2017-03-29 02:35:18.832093: step 200, loss = 7.50 (123.2 examples/sec; 0.519 sec/batch)
2017-03-29 02:35:27.986330: step 210, loss = 7.56 (130.7 examples/sec; 0.490 sec/batch)
2017-03-29 02:35:37.822668: step 220, loss = 7.49 (158.3 examples/sec; 0.404 sec/batch)
2017-03-29 02:35:44.048161: step 230, loss = 7.50 (149.3 examples/sec; 0.429 sec/batch)
2017-03-29 02:35:48.432228: step 240, loss = 7.47 (150.4 examples/sec; 0.425 sec/batch)
2017-03-29 02:35:53.027374: step 250, loss = 7.48 (131.2 examples/sec; 0.488 sec/batch)
2017-03-29 02:35:57.462542: step 260, loss = 7.46 (143.9 examples/sec; 0.445 sec/batch)
2017-03-29 02:36:01.920888: step 270, loss = 7.48 (139.2 examples/sec; 0.460 sec/batch)
2017-03-29 02:36:06.363179: step 280, loss = 7.43 (142.9 examples/sec; 0.448 sec/batch)
2017-03-29 02:36:14.162609: step 290, loss = 7.47 (138.4 examples/sec; 0.462 sec/batch)
2017-03-29 02:36:21.111707: step 300, loss = 7.50 (149.4 examples/sec; 0.428 sec/batch)
2017-03-29 02:36:29.869140: step 310, loss = 7.49 (133.4 examples/sec; 0.480 sec/batch)
2017-03-29 02:36:36.026901: step 320, loss = 7.47 (57.3 examples/sec; 1.118 sec/batch)
2017-03-29 02:36:44.066992: step 330, loss = 7.51 (154.5 examples/sec; 0.414 sec/batch)
2017-03-29 02:36:52.858664: step 340, loss = 7.50 (134.1 examples/sec; 0.477 sec/batch)
2017-03-29 02:37:00.570737: step 350, loss = 7.48 (53.9 examples/sec; 1.188 sec/batch)
2017-03-29 02:37:06.548617: step 360, loss = 7.56 (149.6 examples/sec; 0.428 sec/batch)
2017-03-29 02:37:12.473038: step 370, loss = 7.52 (51.4 examples/sec; 1.245 sec/batch)
2017-03-29 02:37:21.951074: step 380, loss = 7.44 (30.4 examples/sec; 2.106 sec/batch)
2017-03-29 02:37:30.189092: step 390, loss = 7.52 (148.9 examples/sec; 0.430 sec/batch)
2017-03-29 02:37:37.516368: step 400, loss = 7.53 (146.1 examples/sec; 0.438 sec/batch)
2017-03-29 02:37:46.165455: step 410, loss = 7.48 (146.0 examples/sec; 0.438 sec/batch)
2017-03-29 02:37:50.679680: step 420, loss = 7.52 (153.5 examples/sec; 0.417 sec/batch)
2017-03-29 02:37:55.201122: step 430, loss = 7.48 (153.5 examples/sec; 0.417 sec/batch)
2017-03-29 02:37:59.839987: step 440, loss = 7.46 (148.3 examples/sec; 0.431 sec/batch)
2017-03-29 02:38:04.405813: step 450, loss = 7.51 (128.4 examples/sec; 0.499 sec/batch)
2017-03-29 02:38:12.270870: step 460, loss = 7.38 (28.9 examples/sec; 2.213 sec/batch)
2017-03-29 02:38:16.681977: step 470, loss = 7.47 (138.8 examples/sec; 0.461 sec/batch)
2017-03-29 02:38:21.126319: step 480, loss = 7.50 (142.9 examples/sec; 0.448 sec/batch)
2017-03-29 02:38:25.873936: step 490, loss = 7.50 (127.4 examples/sec; 0.502 sec/batch)
2017-03-29 02:38:30.385124: step 500, loss = 7.50 (150.9 examples/sec; 0.424 sec/batch)
2017-03-29 02:38:38.970391: step 510, loss = 7.55 (145.3 examples/sec; 0.440 sec/batch)
2017-03-29 02:38:43.418155: step 520, loss = 7.50 (156.5 examples/sec; 0.409 sec/batch)
2017-03-29 02:38:49.242143: step 530, loss = 7.49 (148.3 examples/sec; 0.431 sec/batch)
2017-03-29 02:38:57.831999: step 540, loss = 7.46 (145.9 examples/sec; 0.439 sec/batch)
2017-03-29 02:39:02.392299: step 550, loss = 7.51 (124.6 examples/sec; 0.514 sec/batch)
2017-03-29 02:39:07.436383: step 560, loss = 7.54 (136.4 examples/sec; 0.469 sec/batch)
2017-03-29 02:39:13.280016: step 570, loss = 7.54 (150.8 examples/sec; 0.424 sec/batch)
2017-03-29 02:39:17.831896: step 580, loss = 7.42 (140.8 examples/sec; 0.454 sec/batch)
2017-03-29 02:39:22.434518: step 590, loss = 7.51 (138.5 examples/sec; 0.462 sec/batch)
2017-03-29 02:39:27.124407: step 600, loss = 7.46 (145.7 examples/sec; 0.439 sec/batch)
2017-03-29 02:39:36.262775: step 610, loss = 7.47 (143.8 examples/sec; 0.445 sec/batch)
2017-03-29 02:39:40.985405: step 620, loss = 7.48 (129.2 examples/sec; 0.495 sec/batch)
2017-03-29 02:39:45.570867: step 630, loss = 7.52 (139.9 examples/sec; 0.457 sec/batch)
2017-03-29 02:39:50.306357: step 640, loss = 7.49 (153.7 examples/sec; 0.417 sec/batch)
2017-03-29 02:39:55.003823: step 650, loss = 7.54 (142.8 examples/sec; 0.448 sec/batch)
2017-03-29 02:39:59.391042: step 660, loss = 7.44 (143.9 examples/sec; 0.445 sec/batch)
2017-03-29 02:40:04.058647: step 670, loss = 7.51 (128.2 examples/sec; 0.499 sec/batch)
2017-03-29 02:40:08.952709: step 680, loss = 7.53 (148.7 examples/sec; 0.431 sec/batch)
2017-03-29 02:40:13.849013: step 690, loss = 7.50 (143.0 examples/sec; 0.447 sec/batch)
2017-03-29 02:40:18.340166: step 700, loss = 7.46 (145.0 examples/sec; 0.441 sec/batch)
2017-03-29 02:40:26.877834: step 710, loss = 7.48 (141.0 examples/sec; 0.454 sec/batch)
2017-03-29 02:40:31.733411: step 720, loss = 7.48 (127.0 examples/sec; 0.504 sec/batch)
2017-03-29 02:40:36.353485: step 730, loss = 7.44 (144.3 examples/sec; 0.443 sec/batch)
2017-03-29 02:40:40.855288: step 740, loss = 7.47 (127.8 examples/sec; 0.501 sec/batch)
2017-03-29 02:40:46.900969: step 750, loss = 7.48 (71.7 examples/sec; 0.893 sec/batch)
2017-03-29 02:40:51.275912: step 760, loss = 7.49 (151.9 examples/sec; 0.421 sec/batch)
2017-03-29 02:40:56.577769: step 770, loss = 7.44 (123.0 examples/sec; 0.520 sec/batch)
2017-03-29 02:41:04.329146: step 780, loss = 7.46 (42.3 examples/sec; 1.514 sec/batch)
2017-03-29 02:41:10.190843: step 790, loss = 7.42 (41.1 examples/sec; 1.559 sec/batch)
2017-03-29 02:41:15.617623: step 800, loss = 7.47 (152.3 examples/sec; 0.420 sec/batch)
2017-03-29 02:41:25.769967: step 810, loss = 7.40 (147.6 examples/sec; 0.434 sec/batch)
2017-03-29 02:41:30.447458: step 820, loss = 7.45 (145.3 examples/sec; 0.440 sec/batch)
2017-03-29 02:41:34.966968: step 830, loss = 7.49 (130.2 examples/sec; 0.492 sec/batch)
2017-03-29 02:41:39.509773: step 840, loss = 7.48 (132.6 examples/sec; 0.483 sec/batch)
2017-03-29 02:41:45.361489: step 850, loss = 7.53 (137.1 examples/sec; 0.467 sec/batch)
2017-03-29 02:41:51.141612: step 860, loss = 7.48 (144.2 examples/sec; 0.444 sec/batch)
2017-03-29 02:41:55.734069: step 870, loss = 7.53 (152.1 examples/sec; 0.421 sec/batch)
2017-03-29 02:42:02.415004: step 880, loss = 7.49 (143.2 examples/sec; 0.447 sec/batch)
2017-03-29 02:42:07.302291: step 890, loss = 7.48 (142.7 examples/sec; 0.449 sec/batch)
2017-03-29 02:42:12.565494: step 900, loss = 7.47 (149.3 examples/sec; 0.429 sec/batch)
2017-03-29 02:42:21.321678: step 910, loss = 7.45 (147.8 examples/sec; 0.433 sec/batch)
2017-03-29 02:42:25.885195: step 920, loss = 7.47 (132.2 examples/sec; 0.484 sec/batch)
2017-03-29 02:42:30.658475: step 930, loss = 7.52 (145.6 examples/sec; 0.440 sec/batch)
2017-03-29 02:42:35.306315: step 940, loss = 7.49 (138.2 examples/sec; 0.463 sec/batch)
2017-03-29 02:42:41.635815: step 950, loss = 7.53 (147.9 examples/sec; 0.433 sec/batch)
2017-03-29 02:42:46.447246: step 960, loss = 7.48 (141.6 examples/sec; 0.452 sec/batch)
2017-03-29 02:42:50.961416: step 970, loss = 7.40 (135.8 examples/sec; 0.471 sec/batch)
2017-03-29 02:42:55.676688: step 980, loss = 7.46 (139.1 examples/sec; 0.460 sec/batch)
2017-03-29 02:43:00.251137: step 990, loss = 7.46 (132.3 examples/sec; 0.484 sec/batch)
2017-03-29 02:43:04.778230: step 1000, loss = 7.45 (144.1 examples/sec; 0.444 sec/batch)
2017-03-29 02:43:13.694571: step 1010, loss = 7.47 (142.5 examples/sec; 0.449 sec/batch)
2017-03-29 02:43:18.403207: step 1020, loss = 7.47 (133.8 examples/sec; 0.478 sec/batch)
2017-03-29 02:43:22.848915: step 1030, loss = 7.49 (151.7 examples/sec; 0.422 sec/batch)
2017-03-29 02:43:27.584285: step 1040, loss = 7.47 (146.7 examples/sec; 0.436 sec/batch)
2017-03-29 02:43:32.119278: step 1050, loss = 7.48 (146.3 examples/sec; 0.437 sec/batch)
2017-03-29 02:43:36.723445: step 1060, loss = 7.50 (128.8 examples/sec; 0.497 sec/batch)
2017-03-29 02:43:41.161787: step 1070, loss = 7.46 (134.4 examples/sec; 0.476 sec/batch)
2017-03-29 02:43:46.536103: step 1080, loss = 7.47 (135.8 examples/sec; 0.471 sec/batch)
2017-03-29 02:43:52.697921: step 1090, loss = 7.52 (145.6 examples/sec; 0.439 sec/batch)
2017-03-29 02:43:57.839895: step 1100, loss = 7.45 (141.9 examples/sec; 0.451 sec/batch)
2017-03-29 02:44:06.411354: step 1110, loss = 7.53 (139.8 examples/sec; 0.458 sec/batch)
2017-03-29 02:44:10.839474: step 1120, loss = 7.49 (144.7 examples/sec; 0.442 sec/batch)
2017-03-29 02:44:15.448210: step 1130, loss = 7.52 (135.3 examples/sec; 0.473 sec/batch)
2017-03-29 02:44:20.038477: step 1140, loss = 7.49 (129.7 examples/sec; 0.493 sec/batch)
2017-03-29 02:44:24.521661: step 1150, loss = 7.54 (134.3 examples/sec; 0.477 sec/batch)
2017-03-29 02:44:29.273772: step 1160, loss = 7.50 (127.2 examples/sec; 0.503 sec/batch)
2017-03-29 02:44:33.714318: step 1170, loss = 7.51 (143.6 examples/sec; 0.446 sec/batch)
2017-03-29 02:44:38.460067: step 1180, loss = 7.48 (151.5 examples/sec; 0.422 sec/batch)
2017-03-29 02:44:42.887537: step 1190, loss = 7.42 (142.2 examples/sec; 0.450 sec/batch)
2017-03-29 02:44:47.449727: step 1200, loss = 7.51 (137.8 examples/sec; 0.464 sec/batch)
2017-03-29 02:44:56.270915: step 1210, loss = 7.50 (124.7 examples/sec; 0.513 sec/batch)
2017-03-29 02:45:00.863070: step 1220, loss = 7.46 (134.1 examples/sec; 0.477 sec/batch)
2017-03-29 02:45:05.626841: step 1230, loss = 7.53 (132.7 examples/sec; 0.482 sec/batch)
2017-03-29 02:45:10.225966: step 1240, loss = 7.53 (149.7 examples/sec; 0.428 sec/batch)
2017-03-29 02:45:14.849485: step 1250, loss = 7.62 (146.4 examples/sec; 0.437 sec/batch)
2017-03-29 02:45:19.433330: step 1260, loss = 7.51 (131.6 examples/sec; 0.486 sec/batch)
2017-03-29 02:45:24.001001: step 1270, loss = 7.49 (138.4 examples/sec; 0.463 sec/batch)
2017-03-29 02:45:28.558121: step 1280, loss = 7.47 (148.1 examples/sec; 0.432 sec/batch)
2017-03-29 02:45:33.145017: step 1290, loss = 7.42 (148.6 examples/sec; 0.431 sec/batch)
2017-03-29 02:45:37.759096: step 1300, loss = 7.56 (143.3 examples/sec; 0.446 sec/batch)
2017-03-29 02:45:46.793309: step 1310, loss = 7.47 (138.3 examples/sec; 0.463 sec/batch)
2017-03-29 02:45:51.373169: step 1320, loss = 7.49 (141.9 examples/sec; 0.451 sec/batch)
2017-03-29 02:45:55.834552: step 1330, loss = 7.43 (152.7 examples/sec; 0.419 sec/batch)
2017-03-29 02:46:00.412529: step 1340, loss = 7.55 (145.5 examples/sec; 0.440 sec/batch)
2017-03-29 02:46:04.898320: step 1350, loss = 7.50 (150.0 examples/sec; 0.427 sec/batch)
2017-03-29 02:46:09.470973: step 1360, loss = 7.49 (147.0 examples/sec; 0.435 sec/batch)
2017-03-29 02:46:14.041208: step 1370, loss = 7.49 (140.1 examples/sec; 0.457 sec/batch)
2017-03-29 02:46:18.655703: step 1380, loss = 7.47 (141.1 examples/sec; 0.454 sec/batch)
2017-03-29 02:46:23.426459: step 1390, loss = 7.55 (128.2 examples/sec; 0.499 sec/batch)
2017-03-29 02:46:28.062184: step 1400, loss = 7.42 (146.6 examples/sec; 0.437 sec/batch)
2017-03-29 02:46:36.520452: step 1410, loss = 7.43 (148.9 examples/sec; 0.430 sec/batch)
2017-03-29 02:46:40.946068: step 1420, loss = 7.49 (131.9 examples/sec; 0.485 sec/batch)
2017-03-29 02:46:45.478879: step 1430, loss = 7.51 (142.4 examples/sec; 0.449 sec/batch)
2017-03-29 02:46:50.047695: step 1440, loss = 7.57 (131.1 examples/sec; 0.488 sec/batch)
2017-03-29 02:46:54.526750: step 1450, loss = 7.50 (128.3 examples/sec; 0.499 sec/batch)
2017-03-29 02:46:59.134859: step 1460, loss = 7.52 (144.4 examples/sec; 0.443 sec/batch)
2017-03-29 02:47:03.821443: step 1470, loss = 7.52 (135.1 examples/sec; 0.474 sec/batch)
2017-03-29 02:47:08.432910: step 1480, loss = 7.47 (149.4 examples/sec; 0.428 sec/batch)
2017-03-29 02:47:13.209922: step 1490, loss = 7.44 (147.6 examples/sec; 0.434 sec/batch)
2017-03-29 02:47:17.694055: step 1500, loss = 7.46 (137.0 examples/sec; 0.467 sec/batch)
2017-03-29 02:47:26.448371: step 1510, loss = 7.50 (144.6 examples/sec; 0.443 sec/batch)
2017-03-29 02:47:31.033669: step 1520, loss = 7.56 (135.9 examples/sec; 0.471 sec/batch)
2017-03-29 02:47:35.639733: step 1530, loss = 7.57 (145.5 examples/sec; 0.440 sec/batch)
2017-03-29 02:47:40.454651: step 1540, loss = 7.47 (136.8 examples/sec; 0.468 sec/batch)
2017-03-29 02:47:45.138066: step 1550, loss = 7.49 (133.8 examples/sec; 0.478 sec/batch)
2017-03-29 02:47:49.822949: step 1560, loss = 7.49 (128.9 examples/sec; 0.496 sec/batch)
2017-03-29 02:47:54.413963: step 1570, loss = 7.44 (142.3 examples/sec; 0.450 sec/batch)
2017-03-29 02:47:59.075407: step 1580, loss = 7.40 (130.1 examples/sec; 0.492 sec/batch)
2017-03-29 02:48:03.665094: step 1590, loss = 7.49 (149.0 examples/sec; 0.430 sec/batch)
2017-03-29 02:48:08.310279: step 1600, loss = 7.48 (125.8 examples/sec; 0.509 sec/batch)
2017-03-29 02:48:17.628954: step 1610, loss = 7.51 (143.8 examples/sec; 0.445 sec/batch)
2017-03-29 02:48:22.150908: step 1620, loss = 7.48 (149.8 examples/sec; 0.427 sec/batch)
2017-03-29 02:48:26.786554: step 1630, loss = 7.49 (130.4 examples/sec; 0.491 sec/batch)
2017-03-29 02:48:31.215168: step 1640, loss = 7.52 (154.0 examples/sec; 0.415 sec/batch)
2017-03-29 02:48:35.857769: step 1650, loss = 7.50 (148.9 examples/sec; 0.430 sec/batch)
2017-03-29 02:48:40.477843: step 1660, loss = 7.48 (128.6 examples/sec; 0.498 sec/batch)
2017-03-29 02:48:45.155633: step 1670, loss = 7.49 (142.6 examples/sec; 0.449 sec/batch)
2017-03-29 02:48:49.656886: step 1680, loss = 7.47 (142.3 examples/sec; 0.450 sec/batch)
2017-03-29 02:48:54.294805: step 1690, loss = 7.47 (148.7 examples/sec; 0.430 sec/batch)
2017-03-29 02:48:58.934102: step 1700, loss = 7.49 (144.2 examples/sec; 0.444 sec/batch)
2017-03-29 02:49:07.741994: step 1710, loss = 7.49 (135.4 examples/sec; 0.473 sec/batch)
2017-03-29 02:49:12.449929: step 1720, loss = 7.46 (135.6 examples/sec; 0.472 sec/batch)
2017-03-29 02:49:17.216782: step 1730, loss = 7.48 (136.1 examples/sec; 0.470 sec/batch)
2017-03-29 02:49:21.713403: step 1740, loss = 7.50 (122.9 examples/sec; 0.521 sec/batch)
2017-03-29 02:49:26.265260: step 1750, loss = 7.46 (135.2 examples/sec; 0.473 sec/batch)
2017-03-29 02:49:30.849796: step 1760, loss = 7.48 (132.8 examples/sec; 0.482 sec/batch)
2017-03-29 02:49:35.431529: step 1770, loss = 7.39 (136.0 examples/sec; 0.471 sec/batch)
2017-03-29 02:49:40.078999: step 1780, loss = 7.52 (132.5 examples/sec; 0.483 sec/batch)
2017-03-29 02:49:44.734357: step 1790, loss = 7.52 (126.2 examples/sec; 0.507 sec/batch)
2017-03-29 02:49:49.290348: step 1800, loss = 7.47 (125.3 examples/sec; 0.511 sec/batch)
2017-03-29 02:49:57.811447: step 1810, loss = 7.48 (143.9 examples/sec; 0.445 sec/batch)
2017-03-29 02:50:02.395675: step 1820, loss = 7.47 (154.3 examples/sec; 0.415 sec/batch)
2017-03-29 02:50:06.873006: step 1830, loss = 7.42 (142.3 examples/sec; 0.450 sec/batch)
2017-03-29 02:50:11.573863: step 1840, loss = 7.50 (136.7 examples/sec; 0.468 sec/batch)
2017-03-29 02:50:16.276795: step 1850, loss = 7.47 (146.7 examples/sec; 0.436 sec/batch)
2017-03-29 02:50:20.983800: step 1860, loss = 7.52 (121.6 examples/sec; 0.526 sec/batch)
2017-03-29 02:50:25.543904: step 1870, loss = 7.44 (152.1 examples/sec; 0.421 sec/batch)
2017-03-29 02:50:30.200463: step 1880, loss = 7.47 (130.6 examples/sec; 0.490 sec/batch)
2017-03-29 02:50:35.027482: step 1890, loss = 7.50 (130.3 examples/sec; 0.491 sec/batch)
2017-03-29 02:50:39.767005: step 1900, loss = 7.47 (138.4 examples/sec; 0.462 sec/batch)
2017-03-29 02:50:48.812863: step 1910, loss = 7.44 (128.3 examples/sec; 0.499 sec/batch)
2017-03-29 02:50:53.401969: step 1920, loss = 7.35 (131.9 examples/sec; 0.485 sec/batch)
2017-03-29 02:50:57.999807: step 1930, loss = 7.43 (132.4 examples/sec; 0.483 sec/batch)
2017-03-29 02:51:02.634831: step 1940, loss = 7.45 (135.5 examples/sec; 0.472 sec/batch)
2017-03-29 02:51:07.250310: step 1950, loss = 7.50 (132.8 examples/sec; 0.482 sec/batch)
2017-03-29 02:51:11.848374: step 1960, loss = 7.43 (154.3 examples/sec; 0.415 sec/batch)
2017-03-29 02:51:16.530997: step 1970, loss = 7.50 (151.9 examples/sec; 0.421 sec/batch)
2017-03-29 02:51:21.242878: step 1980, loss = 7.44 (135.0 examples/sec; 0.474 sec/batch)
2017-03-29 02:51:25.883724: step 1990, loss = 7.51 (139.4 examples/sec; 0.459 sec/batch)
2017-03-29 02:51:30.549418: step 2000, loss = 7.43 (148.4 examples/sec; 0.431 sec/batch)
2017-03-29 02:51:39.642948: step 2010, loss = 7.43 (137.6 examples/sec; 0.465 sec/batch)
2017-03-29 02:51:44.355008: step 2020, loss = 7.48 (150.2 examples/sec; 0.426 sec/batch)
2017-03-29 02:51:49.034193: step 2030, loss = 7.69 (128.2 examples/sec; 0.499 sec/batch)
2017-03-29 02:51:53.766923: step 2040, loss = 7.46 (123.4 examples/sec; 0.519 sec/batch)
2017-03-29 02:51:58.403928: step 2050, loss = 7.45 (138.3 examples/sec; 0.463 sec/batch)
2017-03-29 02:52:03.127541: step 2060, loss = 7.53 (148.0 examples/sec; 0.432 sec/batch)
2017-03-29 02:52:07.889314: step 2070, loss = 7.50 (142.2 examples/sec; 0.450 sec/batch)
2017-03-29 02:52:12.730726: step 2080, loss = 7.49 (134.0 examples/sec; 0.478 sec/batch)
2017-03-29 02:52:17.507430: step 2090, loss = 7.50 (148.7 examples/sec; 0.430 sec/batch)
2017-03-29 02:52:22.245542: step 2100, loss = 7.47 (125.0 examples/sec; 0.512 sec/batch)
2017-03-29 02:52:31.270555: step 2110, loss = 7.56 (126.4 examples/sec; 0.506 sec/batch)
2017-03-29 02:52:36.041965: step 2120, loss = 7.51 (128.4 examples/sec; 0.498 sec/batch)
2017-03-29 02:52:40.894529: step 2130, loss = 7.43 (141.0 examples/sec; 0.454 sec/batch)
2017-03-29 02:52:45.790572: step 2140, loss = 7.46 (135.7 examples/sec; 0.471 sec/batch)
2017-03-29 02:52:50.695792: step 2150, loss = 7.44 (139.7 examples/sec; 0.458 sec/batch)
2017-03-29 02:52:55.583518: step 2160, loss = 7.50 (139.1 examples/sec; 0.460 sec/batch)
2017-03-29 02:53:00.408912: step 2170, loss = 7.42 (127.1 examples/sec; 0.504 sec/batch)
2017-03-29 02:53:05.217999: step 2180, loss = 7.60 (142.3 examples/sec; 0.450 sec/batch)
2017-03-29 02:53:10.144605: step 2190, loss = 7.39 (131.9 examples/sec; 0.485 sec/batch)
2017-03-29 02:53:15.085615: step 2200, loss = 7.44 (128.3 examples/sec; 0.499 sec/batch)
2017-03-29 02:53:24.228203: step 2210, loss = 7.40 (125.0 examples/sec; 0.512 sec/batch)
2017-03-29 02:53:29.035425: step 2220, loss = 7.40 (121.0 examples/sec; 0.529 sec/batch)
2017-03-29 02:53:33.953592: step 2230, loss = 7.45 (124.5 examples/sec; 0.514 sec/batch)
2017-03-29 02:53:38.859513: step 2240, loss = 7.48 (124.0 examples/sec; 0.516 sec/batch)
2017-03-29 02:53:43.616606: step 2250, loss = 7.33 (137.9 examples/sec; 0.464 sec/batch)
2017-03-29 02:53:48.603054: step 2260, loss = 7.47 (136.8 examples/sec; 0.468 sec/batch)
2017-03-29 02:53:53.452792: step 2270, loss = 7.46 (134.3 examples/sec; 0.477 sec/batch)
2017-03-29 02:53:58.240436: step 2280, loss = 7.38 (142.5 examples/sec; 0.449 sec/batch)
2017-03-29 02:54:03.062546: step 2290, loss = 7.52 (114.9 examples/sec; 0.557 sec/batch)
2017-03-29 02:54:07.959958: step 2300, loss = 7.40 (130.6 examples/sec; 0.490 sec/batch)
2017-03-29 02:54:17.204789: step 2310, loss = 7.38 (141.4 examples/sec; 0.453 sec/batch)
2017-03-29 02:54:22.083088: step 2320, loss = 7.44 (134.4 examples/sec; 0.476 sec/batch)
2017-03-29 02:54:27.036628: step 2330, loss = 7.40 (131.4 examples/sec; 0.487 sec/batch)
2017-03-29 02:54:31.934297: step 2340, loss = 7.39 (121.4 examples/sec; 0.527 sec/batch)
2017-03-29 02:54:36.835518: step 2350, loss = 7.42 (125.8 examples/sec; 0.509 sec/batch)
2017-03-29 02:54:41.630045: step 2360, loss = 7.38 (125.5 examples/sec; 0.510 sec/batch)
2017-03-29 02:54:46.313795: step 2370, loss = 7.33 (134.0 examples/sec; 0.478 sec/batch)
2017-03-29 02:54:51.044044: step 2380, loss = 7.41 (122.5 examples/sec; 0.522 sec/batch)
2017-03-29 02:54:55.828643: step 2390, loss = 7.46 (127.8 examples/sec; 0.501 sec/batch)
2017-03-29 02:55:00.631075: step 2400, loss = 7.43 (123.6 examples/sec; 0.518 sec/batch)
2017-03-29 02:55:09.311265: step 2410, loss = 7.42 (123.2 examples/sec; 0.520 sec/batch)
2017-03-29 02:55:14.323365: step 2420, loss = 7.44 (120.0 examples/sec; 0.533 sec/batch)
2017-03-29 02:55:19.102730: step 2430, loss = 7.45 (137.8 examples/sec; 0.464 sec/batch)
2017-03-29 02:55:23.798173: step 2440, loss = 7.40 (132.5 examples/sec; 0.483 sec/batch)
2017-03-29 02:55:28.543428: step 2450, loss = 7.43 (123.8 examples/sec; 0.517 sec/batch)
2017-03-29 02:55:33.247551: step 2460, loss = 7.37 (142.9 examples/sec; 0.448 sec/batch)
2017-03-29 02:55:37.918425: step 2470, loss = 7.50 (145.9 examples/sec; 0.439 sec/batch)
2017-03-29 02:55:42.620429: step 2480, loss = 7.41 (129.7 examples/sec; 0.494 sec/batch)
2017-03-29 02:55:47.434180: step 2490, loss = 7.48 (131.2 examples/sec; 0.488 sec/batch)
2017-03-29 02:55:52.304507: step 2500, loss = 7.48 (137.8 examples/sec; 0.464 sec/batch)
2017-03-29 02:56:01.188946: step 2510, loss = 7.30 (152.6 examples/sec; 0.419 sec/batch)
2017-03-29 02:56:05.826685: step 2520, loss = 7.38 (145.5 examples/sec; 0.440 sec/batch)
2017-03-29 02:56:10.630023: step 2530, loss = 7.43 (128.7 examples/sec; 0.497 sec/batch)
2017-03-29 02:56:15.381755: step 2540, loss = 7.40 (140.3 examples/sec; 0.456 sec/batch)
2017-03-29 02:56:20.212962: step 2550, loss = 7.43 (135.3 examples/sec; 0.473 sec/batch)
2017-03-29 02:56:24.979084: step 2560, loss = 7.30 (132.8 examples/sec; 0.482 sec/batch)
2017-03-29 02:56:29.688365: step 2570, loss = 7.46 (140.0 examples/sec; 0.457 sec/batch)
2017-03-29 02:56:34.361171: step 2580, loss = 7.46 (141.2 examples/sec; 0.453 sec/batch)
2017-03-29 02:56:39.111140: step 2590, loss = 7.41 (140.7 examples/sec; 0.455 sec/batch)
2017-03-29 02:56:43.878357: step 2600, loss = 7.40 (144.6 examples/sec; 0.443 sec/batch)
2017-03-29 02:56:52.537507: step 2610, loss = 7.43 (137.7 examples/sec; 0.465 sec/batch)
2017-03-29 02:56:57.289991: step 2620, loss = 7.43 (143.1 examples/sec; 0.447 sec/batch)
2017-03-29 02:57:02.056196: step 2630, loss = 7.46 (127.3 examples/sec; 0.503 sec/batch)
2017-03-29 02:57:06.901511: step 2640, loss = 7.46 (133.2 examples/sec; 0.481 sec/batch)
2017-03-29 02:57:11.670448: step 2650, loss = 7.43 (143.4 examples/sec; 0.446 sec/batch)
2017-03-29 02:57:16.369390: step 2660, loss = 7.41 (134.9 examples/sec; 0.474 sec/batch)
2017-03-29 02:57:21.055705: step 2670, loss = 7.48 (137.4 examples/sec; 0.466 sec/batch)
2017-03-29 02:57:25.944490: step 2680, loss = 7.39 (132.8 examples/sec; 0.482 sec/batch)
2017-03-29 02:57:30.798592: step 2690, loss = 7.37 (134.7 examples/sec; 0.475 sec/batch)
2017-03-29 02:57:35.303369: step 2700, loss = 7.62 (148.8 examples/sec; 0.430 sec/batch)
2017-03-29 02:57:44.227182: step 2710, loss = 7.37 (134.2 examples/sec; 0.477 sec/batch)
2017-03-29 02:57:48.946364: step 2720, loss = 7.44 (130.4 examples/sec; 0.491 sec/batch)
2017-03-29 02:57:53.667718: step 2730, loss = 7.36 (131.8 examples/sec; 0.486 sec/batch)
2017-03-29 02:57:58.478463: step 2740, loss = 7.44 (137.0 examples/sec; 0.467 sec/batch)
2017-03-29 02:58:03.308402: step 2750, loss = 7.40 (131.8 examples/sec; 0.486 sec/batch)
2017-03-29 02:58:08.107706: step 2760, loss = 7.34 (134.3 examples/sec; 0.477 sec/batch)
2017-03-29 02:58:12.856137: step 2770, loss = 7.51 (140.1 examples/sec; 0.457 sec/batch)
2017-03-29 02:58:17.620922: step 2780, loss = 7.42 (140.2 examples/sec; 0.456 sec/batch)
2017-03-29 02:58:22.353938: step 2790, loss = 7.41 (138.4 examples/sec; 0.462 sec/batch)
2017-03-29 02:58:27.115208: step 2800, loss = 7.43 (149.9 examples/sec; 0.427 sec/batch)
2017-03-29 02:58:36.124185: step 2810, loss = 7.41 (150.2 examples/sec; 0.426 sec/batch)
2017-03-29 02:58:40.985589: step 2820, loss = 7.41 (134.3 examples/sec; 0.477 sec/batch)
2017-03-29 02:58:45.664862: step 2830, loss = 7.41 (128.4 examples/sec; 0.499 sec/batch)
2017-03-29 02:58:50.383815: step 2840, loss = 7.50 (151.1 examples/sec; 0.424 sec/batch)
2017-03-29 02:58:55.550091: step 2850, loss = 7.46 (150.2 examples/sec; 0.426 sec/batch)
2017-03-29 02:59:00.155212: step 2860, loss = 7.39 (143.4 examples/sec; 0.446 sec/batch)
2017-03-29 02:59:04.675196: step 2870, loss = 7.42 (144.7 examples/sec; 0.442 sec/batch)
2017-03-29 02:59:09.397634: step 2880, loss = 7.33 (124.3 examples/sec; 0.515 sec/batch)
2017-03-29 02:59:13.846030: step 2890, loss = 7.52 (140.6 examples/sec; 0.455 sec/batch)
2017-03-29 02:59:20.181071: step 2900, loss = 7.43 (153.7 examples/sec; 0.417 sec/batch)
2017-03-29 02:59:30.312498: step 2910, loss = 7.34 (150.8 examples/sec; 0.425 sec/batch)
2017-03-29 02:59:34.909535: step 2920, loss = 7.44 (145.4 examples/sec; 0.440 sec/batch)
2017-03-29 02:59:39.448539: step 2930, loss = 7.46 (139.5 examples/sec; 0.459 sec/batch)
2017-03-29 02:59:44.181165: step 2940, loss = 7.33 (146.1 examples/sec; 0.438 sec/batch)
2017-03-29 02:59:48.806712: step 2950, loss = 7.35 (138.9 examples/sec; 0.461 sec/batch)
2017-03-29 02:59:53.304276: step 2960, loss = 7.34 (151.1 examples/sec; 0.423 sec/batch)
2017-03-29 02:59:57.821408: step 2970, loss = 7.44 (140.7 examples/sec; 0.455 sec/batch)
2017-03-29 03:00:02.417102: step 2980, loss = 7.45 (143.3 examples/sec; 0.447 sec/batch)
2017-03-29 03:00:07.057920: step 2990, loss = 7.37 (136.9 examples/sec; 0.467 sec/batch)
2017-03-29 03:00:13.470593: step 3000, loss = 7.46 (140.6 examples/sec; 0.455 sec/batch)
2017-03-29 03:00:22.034121: step 3010, loss = 7.37 (136.8 examples/sec; 0.468 sec/batch)
2017-03-29 03:00:26.385867: step 3020, loss = 7.39 (136.0 examples/sec; 0.470 sec/batch)
2017-03-29 03:00:31.014915: step 3030, loss = 7.43 (138.3 examples/sec; 0.463 sec/batch)
2017-03-29 03:00:35.690735: step 3040, loss = 7.46 (143.8 examples/sec; 0.445 sec/batch)
2017-03-29 03:00:40.204195: step 3050, loss = 7.44 (133.7 examples/sec; 0.479 sec/batch)
2017-03-29 03:00:44.875896: step 3060, loss = 7.52 (147.9 examples/sec; 0.433 sec/batch)
2017-03-29 03:00:49.401436: step 3070, loss = 7.43 (137.7 examples/sec; 0.465 sec/batch)
2017-03-29 03:00:53.940970: step 3080, loss = 7.45 (134.7 examples/sec; 0.475 sec/batch)
2017-03-29 03:00:58.523345: step 3090, loss = 7.35 (150.8 examples/sec; 0.424 sec/batch)
2017-03-29 03:01:03.022739: step 3100, loss = 7.44 (147.2 examples/sec; 0.435 sec/batch)
2017-03-29 03:01:11.747560: step 3110, loss = 7.34 (127.0 examples/sec; 0.504 sec/batch)
2017-03-29 03:01:16.355400: step 3120, loss = 7.36 (136.3 examples/sec; 0.470 sec/batch)
2017-03-29 03:01:20.983716: step 3130, loss = 7.28 (135.8 examples/sec; 0.471 sec/batch)
2017-03-29 03:01:25.605145: step 3140, loss = 7.38 (148.2 examples/sec; 0.432 sec/batch)
2017-03-29 03:01:30.221689: step 3150, loss = 7.41 (140.3 examples/sec; 0.456 sec/batch)
2017-03-29 03:01:34.778891: step 3160, loss = 7.39 (134.3 examples/sec; 0.477 sec/batch)
2017-03-29 03:01:39.423483: step 3170, loss = 7.51 (152.4 examples/sec; 0.420 sec/batch)
2017-03-29 03:01:44.071944: step 3180, loss = 7.40 (145.8 examples/sec; 0.439 sec/batch)
2017-03-29 03:01:48.773635: step 3190, loss = 7.40 (123.5 examples/sec; 0.518 sec/batch)
2017-03-29 03:01:53.494442: step 3200, loss = 7.37 (145.9 examples/sec; 0.439 sec/batch)
2017-03-29 03:02:02.324202: step 3210, loss = 7.38 (152.8 examples/sec; 0.419 sec/batch)
2017-03-29 03:02:07.078230: step 3220, loss = 7.38 (131.2 examples/sec; 0.488 sec/batch)
2017-03-29 03:02:11.784565: step 3230, loss = 7.41 (140.6 examples/sec; 0.455 sec/batch)
2017-03-29 03:02:16.194954: step 3240, loss = 7.46 (142.0 examples/sec; 0.451 sec/batch)
2017-03-29 03:02:20.905620: step 3250, loss = 7.37 (134.8 examples/sec; 0.475 sec/batch)
2017-03-29 03:02:25.589030: step 3260, loss = 7.32 (145.8 examples/sec; 0.439 sec/batch)
2017-03-29 03:02:30.147266: step 3270, loss = 7.34 (146.0 examples/sec; 0.438 sec/batch)
2017-03-29 03:02:34.730702: step 3280, loss = 7.46 (139.6 examples/sec; 0.459 sec/batch)
2017-03-29 03:02:39.398021: step 3290, loss = 7.40 (129.8 examples/sec; 0.493 sec/batch)
2017-03-29 03:02:43.948638: step 3300, loss = 7.44 (148.6 examples/sec; 0.431 sec/batch)
2017-03-29 03:02:52.963341: step 3310, loss = 7.37 (142.4 examples/sec; 0.449 sec/batch)
2017-03-29 03:02:57.632019: step 3320, loss = 7.37 (150.3 examples/sec; 0.426 sec/batch)
2017-03-29 03:03:02.200824: step 3330, loss = 7.39 (135.6 examples/sec; 0.472 sec/batch)
2017-03-29 03:03:06.707664: step 3340, loss = 7.39 (153.7 examples/sec; 0.416 sec/batch)
2017-03-29 03:03:11.338754: step 3350, loss = 7.40 (151.8 examples/sec; 0.422 sec/batch)
2017-03-29 03:03:16.004806: step 3360, loss = 7.38 (141.2 examples/sec; 0.453 sec/batch)
2017-03-29 03:03:20.654521: step 3370, loss = 7.42 (146.2 examples/sec; 0.438 sec/batch)
2017-03-29 03:03:25.263772: step 3380, loss = 7.37 (136.2 examples/sec; 0.470 sec/batch)
2017-03-29 03:03:29.958967: step 3390, loss = 7.36 (134.8 examples/sec; 0.475 sec/batch)
2017-03-29 03:03:34.436767: step 3400, loss = 7.42 (152.6 examples/sec; 0.419 sec/batch)
2017-03-29 03:03:43.316092: step 3410, loss = 7.47 (143.9 examples/sec; 0.445 sec/batch)
2017-03-29 03:03:47.900062: step 3420, loss = 7.34 (146.1 examples/sec; 0.438 sec/batch)
2017-03-29 03:03:52.531697: step 3430, loss = 7.28 (142.4 examples/sec; 0.449 sec/batch)
2017-03-29 03:03:57.013331: step 3440, loss = 7.46 (136.2 examples/sec; 0.470 sec/batch)
2017-03-29 03:04:01.441934: step 3450, loss = 7.44 (143.2 examples/sec; 0.447 sec/batch)
2017-03-29 03:04:06.053556: step 3460, loss = 7.38 (155.1 examples/sec; 0.413 sec/batch)
2017-03-29 03:04:10.615109: step 3470, loss = 7.36 (148.4 examples/sec; 0.431 sec/batch)
2017-03-29 03:04:15.118824: step 3480, loss = 7.26 (142.5 examples/sec; 0.449 sec/batch)
2017-03-29 03:04:19.758223: step 3490, loss = 7.30 (142.2 examples/sec; 0.450 sec/batch)
2017-03-29 03:04:24.395902: step 3500, loss = 7.32 (142.7 examples/sec; 0.449 sec/batch)
2017-03-29 03:04:33.285408: step 3510, loss = 7.39 (153.6 examples/sec; 0.417 sec/batch)
2017-03-29 03:04:37.953168: step 3520, loss = 7.37 (128.9 examples/sec; 0.496 sec/batch)
2017-03-29 03:04:42.481272: step 3530, loss = 7.42 (128.9 examples/sec; 0.497 sec/batch)
2017-03-29 03:04:47.192266: step 3540, loss = 7.42 (153.1 examples/sec; 0.418 sec/batch)
2017-03-29 03:04:51.802056: step 3550, loss = 7.37 (127.2 examples/sec; 0.503 sec/batch)
2017-03-29 03:04:56.377478: step 3560, loss = 7.31 (136.2 examples/sec; 0.470 sec/batch)
2017-03-29 03:05:01.035091: step 3570, loss = 7.40 (122.1 examples/sec; 0.524 sec/batch)
2017-03-29 03:05:05.673313: step 3580, loss = 7.50 (149.1 examples/sec; 0.429 sec/batch)
2017-03-29 03:05:10.470549: step 3590, loss = 7.36 (141.4 examples/sec; 0.453 sec/batch)
2017-03-29 03:05:15.259032: step 3600, loss = 7.32 (129.8 examples/sec; 0.493 sec/batch)
2017-03-29 03:05:24.385476: step 3610, loss = 7.32 (136.4 examples/sec; 0.469 sec/batch)
2017-03-29 03:05:29.118074: step 3620, loss = 7.27 (138.2 examples/sec; 0.463 sec/batch)
2017-03-29 03:05:33.685387: step 3630, loss = 7.38 (145.7 examples/sec; 0.439 sec/batch)
2017-03-29 03:05:38.413117: step 3640, loss = 7.28 (131.8 examples/sec; 0.485 sec/batch)
2017-03-29 03:05:43.056359: step 3650, loss = 7.34 (144.3 examples/sec; 0.444 sec/batch)
2017-03-29 03:05:47.636634: step 3660, loss = 7.36 (137.4 examples/sec; 0.466 sec/batch)
2017-03-29 03:05:52.289931: step 3670, loss = 7.28 (138.4 examples/sec; 0.463 sec/batch)
2017-03-29 03:05:57.000768: step 3680, loss = 7.33 (123.6 examples/sec; 0.518 sec/batch)
2017-03-29 03:06:01.655847: step 3690, loss = 7.37 (132.1 examples/sec; 0.485 sec/batch)
2017-03-29 03:06:06.248528: step 3700, loss = 7.42 (132.8 examples/sec; 0.482 sec/batch)
2017-03-29 03:06:15.250134: step 3710, loss = 7.43 (133.4 examples/sec; 0.480 sec/batch)
2017-03-29 03:06:20.147845: step 3720, loss = 7.35 (135.7 examples/sec; 0.472 sec/batch)
2017-03-29 03:06:24.717284: step 3730, loss = 7.39 (141.6 examples/sec; 0.452 sec/batch)
2017-03-29 03:06:29.441169: step 3740, loss = 7.27 (133.3 examples/sec; 0.480 sec/batch)
2017-03-29 03:06:34.032457: step 3750, loss = 7.35 (124.1 examples/sec; 0.516 sec/batch)
2017-03-29 03:06:38.672695: step 3760, loss = 7.28 (131.1 examples/sec; 0.488 sec/batch)
2017-03-29 03:06:43.325480: step 3770, loss = 7.32 (138.1 examples/sec; 0.463 sec/batch)
2017-03-29 03:06:48.233177: step 3780, loss = 7.41 (125.9 examples/sec; 0.508 sec/batch)
2017-03-29 03:06:53.018042: step 3790, loss = 7.23 (132.2 examples/sec; 0.484 sec/batch)
2017-03-29 03:06:57.733806: step 3800, loss = 7.30 (135.0 examples/sec; 0.474 sec/batch)
2017-03-29 03:07:06.691972: step 3810, loss = 7.41 (143.0 examples/sec; 0.448 sec/batch)
2017-03-29 03:07:11.293571: step 3820, loss = 7.31 (132.0 examples/sec; 0.485 sec/batch)
2017-03-29 03:07:15.970678: step 3830, loss = 7.30 (145.1 examples/sec; 0.441 sec/batch)
2017-03-29 03:07:20.666560: step 3840, loss = 7.31 (147.5 examples/sec; 0.434 sec/batch)
2017-03-29 03:07:25.303307: step 3850, loss = 7.26 (143.6 examples/sec; 0.446 sec/batch)
2017-03-29 03:07:29.955576: step 3860, loss = 7.31 (133.7 examples/sec; 0.479 sec/batch)
2017-03-29 03:07:34.743994: step 3870, loss = 7.34 (125.7 examples/sec; 0.509 sec/batch)
2017-03-29 03:07:39.502856: step 3880, loss = 7.33 (134.7 examples/sec; 0.475 sec/batch)
2017-03-29 03:07:44.199308: step 3890, loss = 7.43 (134.9 examples/sec; 0.475 sec/batch)
2017-03-29 03:07:48.836012: step 3900, loss = 7.37 (139.0 examples/sec; 0.460 sec/batch)
2017-03-29 03:07:57.920874: step 3910, loss = 7.30 (131.6 examples/sec; 0.486 sec/batch)
2017-03-29 03:08:02.721693: step 3920, loss = 7.34 (138.5 examples/sec; 0.462 sec/batch)
2017-03-29 03:08:07.570317: step 3930, loss = 7.31 (129.8 examples/sec; 0.493 sec/batch)
2017-03-29 03:08:12.270365: step 3940, loss = 7.26 (139.2 examples/sec; 0.460 sec/batch)
2017-03-29 03:08:16.951269: step 3950, loss = 7.32 (143.5 examples/sec; 0.446 sec/batch)
2017-03-29 03:08:21.691764: step 3960, loss = 7.42 (126.8 examples/sec; 0.505 sec/batch)
2017-03-29 03:08:26.319712: step 3970, loss = 7.20 (134.0 examples/sec; 0.478 sec/batch)
2017-03-29 03:08:31.116761: step 3980, loss = 7.36 (146.9 examples/sec; 0.436 sec/batch)
2017-03-29 03:08:35.829193: step 3990, loss = 7.37 (130.0 examples/sec; 0.492 sec/batch)
2017-03-29 03:08:40.579976: step 4000, loss = 7.36 (131.1 examples/sec; 0.488 sec/batch)
2017-03-29 03:08:49.712010: step 4010, loss = 7.35 (126.0 examples/sec; 0.508 sec/batch)
2017-03-29 03:08:54.368007: step 4020, loss = 7.33 (129.8 examples/sec; 0.493 sec/batch)
2017-03-29 03:08:59.092159: step 4030, loss = 7.26 (140.2 examples/sec; 0.457 sec/batch)
2017-03-29 03:09:03.846829: step 4040, loss = 7.26 (143.6 examples/sec; 0.446 sec/batch)
2017-03-29 03:09:08.513458: step 4050, loss = 7.20 (126.6 examples/sec; 0.506 sec/batch)
2017-03-29 03:09:13.096052: step 4060, loss = 7.34 (139.8 examples/sec; 0.458 sec/batch)
2017-03-29 03:09:17.896898: step 4070, loss = 7.31 (138.9 examples/sec; 0.461 sec/batch)
2017-03-29 03:09:22.577992: step 4080, loss = 7.41 (133.0 examples/sec; 0.481 sec/batch)
2017-03-29 03:09:27.266966: step 4090, loss = 7.32 (130.5 examples/sec; 0.490 sec/batch)
2017-03-29 03:09:31.886510: step 4100, loss = 7.28 (149.0 examples/sec; 0.430 sec/batch)
2017-03-29 03:09:40.645198: step 4110, loss = 7.23 (128.5 examples/sec; 0.498 sec/batch)
2017-03-29 03:09:45.459046: step 4120, loss = 7.36 (128.3 examples/sec; 0.499 sec/batch)
2017-03-29 03:09:50.148007: step 4130, loss = 7.43 (134.7 examples/sec; 0.475 sec/batch)
2017-03-29 03:09:54.817965: step 4140, loss = 7.31 (147.5 examples/sec; 0.434 sec/batch)
2017-03-29 03:09:59.481332: step 4150, loss = 7.36 (144.0 examples/sec; 0.445 sec/batch)
2017-03-29 03:10:04.259589: step 4160, loss = 7.31 (135.2 examples/sec; 0.473 sec/batch)
2017-03-29 03:10:08.869129: step 4170, loss = 7.33 (138.3 examples/sec; 0.463 sec/batch)
2017-03-29 03:10:13.469848: step 4180, loss = 7.37 (143.6 examples/sec; 0.446 sec/batch)
2017-03-29 03:10:18.122377: step 4190, loss = 7.31 (135.5 examples/sec; 0.472 sec/batch)
2017-03-29 03:10:22.847707: step 4200, loss = 7.41 (119.8 examples/sec; 0.534 sec/batch)
2017-03-29 03:10:32.375072: step 4210, loss = 7.31 (125.8 examples/sec; 0.509 sec/batch)
2017-03-29 03:10:37.050944: step 4220, loss = 7.33 (152.2 examples/sec; 0.420 sec/batch)
2017-03-29 03:10:41.818060: step 4230, loss = 7.29 (136.4 examples/sec; 0.469 sec/batch)
2017-03-29 03:10:46.540554: step 4240, loss = 7.24 (150.1 examples/sec; 0.426 sec/batch)
2017-03-29 03:10:51.203435: step 4250, loss = 7.32 (148.2 examples/sec; 0.432 sec/batch)
2017-03-29 03:10:55.868162: step 4260, loss = 7.43 (149.7 examples/sec; 0.427 sec/batch)
2017-03-29 03:11:00.567192: step 4270, loss = 7.32 (130.9 examples/sec; 0.489 sec/batch)
2017-03-29 03:11:05.319308: step 4280, loss = 7.30 (124.7 examples/sec; 0.513 sec/batch)
2017-03-29 03:11:09.978931: step 4290, loss = 7.33 (149.4 examples/sec; 0.428 sec/batch)
2017-03-29 03:11:14.726301: step 4300, loss = 7.39 (134.0 examples/sec; 0.478 sec/batch)
2017-03-29 03:11:24.056803: step 4310, loss = 7.27 (136.8 examples/sec; 0.468 sec/batch)
2017-03-29 03:11:28.702734: step 4320, loss = 7.31 (152.5 examples/sec; 0.420 sec/batch)
2017-03-29 03:11:33.362294: step 4330, loss = 7.35 (130.1 examples/sec; 0.492 sec/batch)
2017-03-29 03:11:37.919684: step 4340, loss = 7.32 (147.9 examples/sec; 0.433 sec/batch)
2017-03-29 03:11:42.625242: step 4350, loss = 7.30 (132.1 examples/sec; 0.485 sec/batch)
2017-03-29 03:11:47.243024: step 4360, loss = 7.32 (137.4 examples/sec; 0.466 sec/batch)
2017-03-29 03:11:51.935057: step 4370, loss = 7.39 (132.9 examples/sec; 0.482 sec/batch)
2017-03-29 03:11:57.386004: step 4380, loss = 7.44 (134.3 examples/sec; 0.476 sec/batch)
2017-03-29 03:12:01.956345: step 4390, loss = 7.36 (128.5 examples/sec; 0.498 sec/batch)
2017-03-29 03:12:06.603597: step 4400, loss = 7.30 (145.1 examples/sec; 0.441 sec/batch)
2017-03-29 03:12:15.598178: step 4410, loss = 7.19 (151.9 examples/sec; 0.421 sec/batch)
2017-03-29 03:12:20.169804: step 4420, loss = 7.21 (148.3 examples/sec; 0.432 sec/batch)
2017-03-29 03:12:24.757131: step 4430, loss = 7.23 (133.5 examples/sec; 0.479 sec/batch)
2017-03-29 03:12:29.281676: step 4440, loss = 7.15 (127.4 examples/sec; 0.502 sec/batch)
2017-03-29 03:12:34.535204: step 4450, loss = 7.37 (155.4 examples/sec; 0.412 sec/batch)
2017-03-29 03:12:40.178935: step 4460, loss = 7.29 (155.2 examples/sec; 0.412 sec/batch)
2017-03-29 03:12:48.666399: step 4470, loss = 7.20 (147.9 examples/sec; 0.433 sec/batch)
2017-03-29 03:12:56.298957: step 4480, loss = 7.36 (136.7 examples/sec; 0.468 sec/batch)
2017-03-29 03:13:01.839678: step 4490, loss = 7.19 (44.1 examples/sec; 1.453 sec/batch)
2017-03-29 03:13:09.801015: step 4500, loss = 7.35 (143.6 examples/sec; 0.446 sec/batch)
2017-03-29 03:13:18.827296: step 4510, loss = 7.30 (135.4 examples/sec; 0.473 sec/batch)
2017-03-29 03:13:23.427590: step 4520, loss = 7.13 (139.0 examples/sec; 0.461 sec/batch)
2017-03-29 03:13:28.123248: step 4530, loss = 7.22 (140.0 examples/sec; 0.457 sec/batch)
2017-03-29 03:13:32.704837: step 4540, loss = 7.40 (124.2 examples/sec; 0.515 sec/batch)
2017-03-29 03:13:37.459943: step 4550, loss = 7.24 (142.3 examples/sec; 0.450 sec/batch)
2017-03-29 03:13:42.148747: step 4560, loss = 7.30 (134.7 examples/sec; 0.475 sec/batch)
2017-03-29 03:13:46.707590: step 4570, loss = 7.25 (136.3 examples/sec; 0.469 sec/batch)
2017-03-29 03:13:51.233390: step 4580, loss = 7.29 (130.6 examples/sec; 0.490 sec/batch)
2017-03-29 03:13:55.772687: step 4590, loss = 7.21 (142.8 examples/sec; 0.448 sec/batch)
2017-03-29 03:14:02.263092: step 4600, loss = 7.24 (37.5 examples/sec; 1.705 sec/batch)
2017-03-29 03:14:11.243874: step 4610, loss = 7.24 (153.4 examples/sec; 0.417 sec/batch)
2017-03-29 03:14:21.415352: step 4620, loss = 7.18 (138.2 examples/sec; 0.463 sec/batch)
2017-03-29 03:14:25.866693: step 4630, loss = 7.31 (142.2 examples/sec; 0.450 sec/batch)
2017-03-29 03:14:31.483323: step 4640, loss = 7.28 (141.2 examples/sec; 0.453 sec/batch)
2017-03-29 03:14:42.995344: step 4650, loss = 7.33 (32.7 examples/sec; 1.959 sec/batch)
2017-03-29 03:14:49.921393: step 4660, loss = 7.24 (145.2 examples/sec; 0.441 sec/batch)
2017-03-29 03:14:54.442546: step 4670, loss = 7.27 (153.7 examples/sec; 0.416 sec/batch)
2017-03-29 03:14:58.797863: step 4680, loss = 7.28 (152.8 examples/sec; 0.419 sec/batch)
2017-03-29 03:15:03.383567: step 4690, loss = 7.39 (138.8 examples/sec; 0.461 sec/batch)
2017-03-29 03:15:07.985076: step 4700, loss = 7.15 (149.8 examples/sec; 0.427 sec/batch)
2017-03-29 03:15:16.876360: step 4710, loss = 7.25 (124.5 examples/sec; 0.514 sec/batch)
2017-03-29 03:15:21.809010: step 4720, loss = 7.21 (99.3 examples/sec; 0.644 sec/batch)
2017-03-29 03:15:26.456781: step 4730, loss = 7.19 (151.1 examples/sec; 0.424 sec/batch)
2017-03-29 03:15:33.079674: step 4740, loss = 7.31 (146.5 examples/sec; 0.437 sec/batch)
2017-03-29 03:15:37.740920: step 4750, loss = 7.32 (130.9 examples/sec; 0.489 sec/batch)
2017-03-29 03:15:42.208938: step 4760, loss = 7.25 (132.5 examples/sec; 0.483 sec/batch)
2017-03-29 03:15:46.788059: step 4770, loss = 7.27 (142.4 examples/sec; 0.449 sec/batch)
2017-03-29 03:15:51.160303: step 4780, loss = 7.13 (143.6 examples/sec; 0.446 sec/batch)
2017-03-29 03:15:55.784618: step 4790, loss = 7.24 (143.4 examples/sec; 0.446 sec/batch)
2017-03-29 03:16:00.315544: step 4800, loss = 7.16 (149.3 examples/sec; 0.429 sec/batch)
2017-03-29 03:16:09.771937: step 4810, loss = 7.23 (126.8 examples/sec; 0.505 sec/batch)
2017-03-29 03:16:14.567023: step 4820, loss = 7.24 (146.1 examples/sec; 0.438 sec/batch)
2017-03-29 03:16:22.172947: step 4830, loss = 7.30 (137.7 examples/sec; 0.465 sec/batch)
2017-03-29 03:16:26.660669: step 4840, loss = 7.27 (152.8 examples/sec; 0.419 sec/batch)
2017-03-29 03:16:31.105398: step 4850, loss = 7.18 (133.3 examples/sec; 0.480 sec/batch)
2017-03-29 03:16:35.648713: step 4860, loss = 7.24 (137.0 examples/sec; 0.467 sec/batch)
2017-03-29 03:16:40.143248: step 4870, loss = 7.34 (137.8 examples/sec; 0.465 sec/batch)
2017-03-29 03:16:44.819783: step 4880, loss = 7.17 (133.4 examples/sec; 0.480 sec/batch)
2017-03-29 03:16:49.528500: step 4890, loss = 7.24 (148.5 examples/sec; 0.431 sec/batch)
2017-03-29 03:16:54.486207: step 4900, loss = 7.25 (146.4 examples/sec; 0.437 sec/batch)
2017-03-29 03:17:03.711824: step 4910, loss = 7.30 (125.5 examples/sec; 0.510 sec/batch)
2017-03-29 03:17:08.369065: step 4920, loss = 7.25 (139.4 examples/sec; 0.459 sec/batch)
2017-03-29 03:17:12.981791: step 4930, loss = 7.14 (137.6 examples/sec; 0.465 sec/batch)
2017-03-29 03:17:17.711761: step 4940, loss = 7.29 (131.9 examples/sec; 0.485 sec/batch)
2017-03-29 03:17:22.525033: step 4950, loss = 7.15 (79.0 examples/sec; 0.810 sec/batch)
2017-03-29 03:17:28.308889: step 4960, loss = 7.23 (147.5 examples/sec; 0.434 sec/batch)
2017-03-29 03:17:32.878809: step 4970, loss = 7.22 (148.6 examples/sec; 0.431 sec/batch)
2017-03-29 03:17:37.597176: step 4980, loss = 7.30 (139.1 examples/sec; 0.460 sec/batch)
2017-03-29 03:17:42.004022: step 4990, loss = 7.06 (146.3 examples/sec; 0.438 sec/batch)
2017-03-29 03:17:46.436914: step 5000, loss = 7.25 (145.3 examples/sec; 0.440 sec/batch)
2017-03-29 03:18:03.386037: step 5010, loss = 7.24 (141.9 examples/sec; 0.451 sec/batch)
2017-03-29 03:18:08.019754: step 5020, loss = 7.18 (124.9 examples/sec; 0.513 sec/batch)
2017-03-29 03:18:12.588068: step 5030, loss = 7.27 (135.8 examples/sec; 0.471 sec/batch)
2017-03-29 03:18:17.221434: step 5040, loss = 7.18 (128.9 examples/sec; 0.497 sec/batch)
2017-03-29 03:18:21.703502: step 5050, loss = 7.19 (148.1 examples/sec; 0.432 sec/batch)
2017-03-29 03:18:26.283343: step 5060, loss = 7.18 (148.2 examples/sec; 0.432 sec/batch)
2017-03-29 03:18:37.036631: step 5070, loss = 7.15 (13.0 examples/sec; 4.942 sec/batch)
2017-03-29 03:18:42.279740: step 5080, loss = 7.02 (140.7 examples/sec; 0.455 sec/batch)
2017-03-29 03:18:46.783103: step 5090, loss = 7.15 (147.7 examples/sec; 0.433 sec/batch)
2017-03-29 03:18:51.362585: step 5100, loss = 7.11 (147.5 examples/sec; 0.434 sec/batch)
2017-03-29 03:19:00.127894: step 5110, loss = 7.09 (147.3 examples/sec; 0.434 sec/batch)
2017-03-29 03:19:04.666797: step 5120, loss = 6.96 (148.2 examples/sec; 0.432 sec/batch)
2017-03-29 03:19:09.235872: step 5130, loss = 7.14 (128.4 examples/sec; 0.498 sec/batch)
2017-03-29 03:19:13.915162: step 5140, loss = 7.26 (142.6 examples/sec; 0.449 sec/batch)
2017-03-29 03:19:18.450484: step 5150, loss = 7.16 (154.4 examples/sec; 0.415 sec/batch)
2017-03-29 03:19:23.189998: step 5160, loss = 7.06 (125.1 examples/sec; 0.512 sec/batch)
2017-03-29 03:19:27.927367: step 5170, loss = 7.21 (137.7 examples/sec; 0.465 sec/batch)
2017-03-29 03:19:32.554827: step 5180, loss = 7.13 (142.0 examples/sec; 0.451 sec/batch)
2017-03-29 03:19:36.967519: step 5190, loss = 7.31 (144.2 examples/sec; 0.444 sec/batch)
2017-03-29 03:19:41.435738: step 5200, loss = 7.33 (145.1 examples/sec; 0.441 sec/batch)
2017-03-29 03:19:50.145249: step 5210, loss = 7.18 (149.8 examples/sec; 0.427 sec/batch)
2017-03-29 03:19:54.852006: step 5220, loss = 7.19 (151.4 examples/sec; 0.423 sec/batch)
2017-03-29 03:19:59.319358: step 5230, loss = 7.11 (145.9 examples/sec; 0.439 sec/batch)
2017-03-29 03:20:03.788072: step 5240, loss = 7.36 (139.6 examples/sec; 0.459 sec/batch)
2017-03-29 03:20:08.228015: step 5250, loss = 7.06 (147.3 examples/sec; 0.434 sec/batch)
2017-03-29 03:20:12.755902: step 5260, loss = 7.23 (141.0 examples/sec; 0.454 sec/batch)
2017-03-29 03:20:17.389755: step 5270, loss = 7.13 (146.2 examples/sec; 0.438 sec/batch)
2017-03-29 03:20:22.013496: step 5280, loss = 7.18 (154.1 examples/sec; 0.415 sec/batch)
2017-03-29 03:20:26.637746: step 5290, loss = 7.15 (126.3 examples/sec; 0.507 sec/batch)
2017-03-29 03:20:31.143685: step 5300, loss = 7.13 (155.7 examples/sec; 0.411 sec/batch)
2017-03-29 03:20:39.584564: step 5310, loss = 7.10 (135.2 examples/sec; 0.473 sec/batch)
2017-03-29 03:20:43.992770: step 5320, loss = 7.20 (142.1 examples/sec; 0.451 sec/batch)
2017-03-29 03:20:48.529438: step 5330, loss = 7.06 (139.1 examples/sec; 0.460 sec/batch)
2017-03-29 03:20:53.008903: step 5340, loss = 7.14 (145.4 examples/sec; 0.440 sec/batch)
2017-03-29 03:20:57.534772: step 5350, loss = 7.21 (148.4 examples/sec; 0.431 sec/batch)
2017-03-29 03:21:02.223884: step 5360, loss = 7.10 (144.7 examples/sec; 0.442 sec/batch)
2017-03-29 03:21:06.728306: step 5370, loss = 7.12 (145.7 examples/sec; 0.439 sec/batch)
2017-03-29 03:21:11.217240: step 5380, loss = 7.18 (149.4 examples/sec; 0.428 sec/batch)
2017-03-29 03:21:15.772473: step 5390, loss = 7.17 (144.0 examples/sec; 0.445 sec/batch)
2017-03-29 03:21:20.404101: step 5400, loss = 7.15 (132.2 examples/sec; 0.484 sec/batch)
2017-03-29 03:21:29.208379: step 5410, loss = 7.07 (140.1 examples/sec; 0.457 sec/batch)
2017-03-29 03:21:33.731051: step 5420, loss = 7.03 (145.5 examples/sec; 0.440 sec/batch)
2017-03-29 03:21:38.219700: step 5430, loss = 7.13 (144.8 examples/sec; 0.442 sec/batch)
2017-03-29 03:21:42.736556: step 5440, loss = 7.14 (138.4 examples/sec; 0.463 sec/batch)
2017-03-29 03:21:47.270846: step 5450, loss = 7.23 (146.2 examples/sec; 0.438 sec/batch)
2017-03-29 03:21:51.792189: step 5460, loss = 7.07 (145.4 examples/sec; 0.440 sec/batch)
2017-03-29 03:21:56.475273: step 5470, loss = 7.04 (131.9 examples/sec; 0.485 sec/batch)
2017-03-29 03:22:00.940222: step 5480, loss = 7.10 (146.3 examples/sec; 0.438 sec/batch)
2017-03-29 03:22:05.569474: step 5490, loss = 7.11 (148.3 examples/sec; 0.431 sec/batch)
2017-03-29 03:22:10.258107: step 5500, loss = 7.24 (140.3 examples/sec; 0.456 sec/batch)
2017-03-29 03:22:18.963790: step 5510, loss = 6.98 (145.1 examples/sec; 0.441 sec/batch)
2017-03-29 03:22:23.729213: step 5520, loss = 6.94 (147.2 examples/sec; 0.435 sec/batch)
2017-03-29 03:22:28.475744: step 5530, loss = 7.16 (129.9 examples/sec; 0.493 sec/batch)
2017-03-29 03:22:33.223699: step 5540, loss = 7.12 (126.7 examples/sec; 0.505 sec/batch)
2017-03-29 03:22:37.710069: step 5550, loss = 7.15 (134.0 examples/sec; 0.477 sec/batch)
2017-03-29 03:22:42.345113: step 5560, loss = 7.14 (139.9 examples/sec; 0.457 sec/batch)
2017-03-29 03:22:47.059238: step 5570, loss = 7.14 (140.5 examples/sec; 0.456 sec/batch)
2017-03-29 03:22:51.595123: step 5580, loss = 7.07 (143.2 examples/sec; 0.447 sec/batch)
2017-03-29 03:22:56.322461: step 5590, loss = 6.95 (146.6 examples/sec; 0.437 sec/batch)
2017-03-29 03:23:00.985536: step 5600, loss = 7.11 (129.6 examples/sec; 0.494 sec/batch)
2017-03-29 03:23:10.173479: step 5610, loss = 7.09 (136.0 examples/sec; 0.471 sec/batch)
2017-03-29 03:23:14.859181: step 5620, loss = 7.18 (127.6 examples/sec; 0.502 sec/batch)
2017-03-29 03:23:19.346338: step 5630, loss = 6.85 (147.8 examples/sec; 0.433 sec/batch)
2017-03-29 03:23:23.895157: step 5640, loss = 6.81 (143.7 examples/sec; 0.445 sec/batch)
2017-03-29 03:23:28.427663: step 5650, loss = 7.10 (143.2 examples/sec; 0.447 sec/batch)
2017-03-29 03:23:33.003175: step 5660, loss = 7.06 (126.0 examples/sec; 0.508 sec/batch)
2017-03-29 03:23:37.667332: step 5670, loss = 6.80 (139.9 examples/sec; 0.458 sec/batch)
2017-03-29 03:23:42.274706: step 5680, loss = 6.93 (148.1 examples/sec; 0.432 sec/batch)
2017-03-29 03:23:46.934188: step 5690, loss = 7.10 (148.3 examples/sec; 0.432 sec/batch)
2017-03-29 03:23:51.609501: step 5700, loss = 7.01 (133.3 examples/sec; 0.480 sec/batch)
2017-03-29 03:24:00.667448: step 5710, loss = 7.09 (120.2 examples/sec; 0.533 sec/batch)
2017-03-29 03:24:05.304683: step 5720, loss = 7.18 (134.0 examples/sec; 0.478 sec/batch)
2017-03-29 03:24:09.958616: step 5730, loss = 7.22 (142.4 examples/sec; 0.449 sec/batch)
2017-03-29 03:24:14.649565: step 5740, loss = 7.15 (133.0 examples/sec; 0.481 sec/batch)
2017-03-29 03:24:19.178726: step 5750, loss = 7.03 (147.2 examples/sec; 0.435 sec/batch)
2017-03-29 03:24:23.894938: step 5760, loss = 7.10 (124.1 examples/sec; 0.516 sec/batch)
2017-03-29 03:24:28.516622: step 5770, loss = 7.20 (144.9 examples/sec; 0.442 sec/batch)
2017-03-29 03:24:33.163715: step 5780, loss = 7.19 (135.4 examples/sec; 0.473 sec/batch)
2017-03-29 03:24:37.891694: step 5790, loss = 7.14 (123.4 examples/sec; 0.519 sec/batch)
2017-03-29 03:24:42.619828: step 5800, loss = 7.06 (135.0 examples/sec; 0.474 sec/batch)
2017-03-29 03:24:52.032187: step 5810, loss = 6.94 (126.9 examples/sec; 0.504 sec/batch)
2017-03-29 03:24:56.681086: step 5820, loss = 7.11 (135.1 examples/sec; 0.474 sec/batch)
2017-03-29 03:25:01.333966: step 5830, loss = 7.09 (134.1 examples/sec; 0.477 sec/batch)
2017-03-29 03:25:05.958605: step 5840, loss = 7.00 (136.6 examples/sec; 0.468 sec/batch)
2017-03-29 03:25:10.702577: step 5850, loss = 7.07 (126.5 examples/sec; 0.506 sec/batch)
2017-03-29 03:25:15.347939: step 5860, loss = 6.99 (147.4 examples/sec; 0.434 sec/batch)
2017-03-29 03:25:19.969656: step 5870, loss = 7.20 (150.7 examples/sec; 0.425 sec/batch)
2017-03-29 03:25:24.783961: step 5880, loss = 7.03 (129.9 examples/sec; 0.493 sec/batch)
2017-03-29 03:25:29.624190: step 5890, loss = 7.23 (130.5 examples/sec; 0.491 sec/batch)
2017-03-29 03:25:34.184991: step 5900, loss = 7.00 (142.2 examples/sec; 0.450 sec/batch)
2017-03-29 03:25:43.084151: step 5910, loss = 7.10 (149.4 examples/sec; 0.428 sec/batch)
2017-03-29 03:25:47.787382: step 5920, loss = 7.22 (140.3 examples/sec; 0.456 sec/batch)
2017-03-29 03:25:52.580057: step 5930, loss = 7.08 (140.9 examples/sec; 0.454 sec/batch)
2017-03-29 03:25:59.417526: step 5940, loss = 7.32 (143.0 examples/sec; 0.448 sec/batch)
2017-03-29 03:26:04.002549: step 5950, loss = 7.19 (144.9 examples/sec; 0.442 sec/batch)
2017-03-29 03:26:08.515667: step 5960, loss = 7.04 (152.3 examples/sec; 0.420 sec/batch)
2017-03-29 03:26:13.151009: step 5970, loss = 7.17 (145.1 examples/sec; 0.441 sec/batch)
2017-03-29 03:26:17.812536: step 5980, loss = 7.05 (135.9 examples/sec; 0.471 sec/batch)
2017-03-29 03:26:22.255804: step 5990, loss = 6.91 (147.8 examples/sec; 0.433 sec/batch)
2017-03-29 03:26:33.663728: step 6000, loss = 7.13 (42.2 examples/sec; 1.518 sec/batch)
2017-03-29 03:26:42.410723: step 6010, loss = 7.02 (141.5 examples/sec; 0.452 sec/batch)
2017-03-29 03:26:47.010793: step 6020, loss = 6.99 (134.1 examples/sec; 0.477 sec/batch)
2017-03-29 03:26:51.530374: step 6030, loss = 7.07 (140.8 examples/sec; 0.455 sec/batch)
2017-03-29 03:27:01.594228: step 6040, loss = 7.04 (39.5 examples/sec; 1.621 sec/batch)
2017-03-29 03:27:08.387193: step 6050, loss = 7.11 (142.2 examples/sec; 0.450 sec/batch)
2017-03-29 03:27:13.885566: step 6060, loss = 7.05 (140.9 examples/sec; 0.454 sec/batch)
2017-03-29 03:27:21.760418: step 6070, loss = 7.04 (135.3 examples/sec; 0.473 sec/batch)
2017-03-29 03:27:26.253008: step 6080, loss = 7.09 (141.6 examples/sec; 0.452 sec/batch)
2017-03-29 03:27:30.794878: step 6090, loss = 6.90 (152.9 examples/sec; 0.419 sec/batch)
2017-03-29 03:27:37.339000: step 6100, loss = 6.84 (151.3 examples/sec; 0.423 sec/batch)
2017-03-29 03:27:45.975360: step 6110, loss = 7.07 (135.9 examples/sec; 0.471 sec/batch)
2017-03-29 03:27:50.695402: step 6120, loss = 7.10 (143.5 examples/sec; 0.446 sec/batch)
2017-03-29 03:27:55.128205: step 6130, loss = 7.18 (144.0 examples/sec; 0.444 sec/batch)
2017-03-29 03:28:01.068406: step 6140, loss = 7.02 (147.1 examples/sec; 0.435 sec/batch)
2017-03-29 03:28:06.294213: step 6150, loss = 7.02 (151.4 examples/sec; 0.423 sec/batch)
2017-03-29 03:28:10.816866: step 6160, loss = 7.05 (142.9 examples/sec; 0.448 sec/batch)
2017-03-29 03:28:16.995419: step 6170, loss = 7.08 (144.2 examples/sec; 0.444 sec/batch)
2017-03-29 03:28:22.207858: step 6180, loss = 7.11 (145.2 examples/sec; 0.441 sec/batch)
2017-03-29 03:28:26.834737: step 6190, loss = 7.27 (140.9 examples/sec; 0.454 sec/batch)
2017-03-29 03:28:31.943223: step 6200, loss = 6.96 (84.1 examples/sec; 0.761 sec/batch)
2017-03-29 03:28:41.102052: step 6210, loss = 6.74 (100.1 examples/sec; 0.639 sec/batch)
2017-03-29 03:28:45.753397: step 6220, loss = 7.04 (144.0 examples/sec; 0.444 sec/batch)
2017-03-29 03:28:50.286626: step 6230, loss = 7.12 (149.4 examples/sec; 0.428 sec/batch)
2017-03-29 03:28:54.714662: step 6240, loss = 7.21 (132.9 examples/sec; 0.481 sec/batch)
2017-03-29 03:29:03.033249: step 6250, loss = 7.03 (144.9 examples/sec; 0.442 sec/batch)
2017-03-29 03:29:07.983452: step 6260, loss = 6.99 (98.1 examples/sec; 0.652 sec/batch)
2017-03-29 03:29:14.499119: step 6270, loss = 7.16 (36.2 examples/sec; 1.766 sec/batch)
2017-03-29 03:29:21.471414: step 6280, loss = 7.15 (145.0 examples/sec; 0.442 sec/batch)
2017-03-29 03:29:26.754496: step 6290, loss = 7.18 (152.7 examples/sec; 0.419 sec/batch)
2017-03-29 03:29:31.906106: step 6300, loss = 7.10 (150.7 examples/sec; 0.425 sec/batch)
2017-03-29 03:29:40.534338: step 6310, loss = 7.02 (146.3 examples/sec; 0.438 sec/batch)
2017-03-29 03:29:45.130526: step 6320, loss = 7.03 (143.9 examples/sec; 0.445 sec/batch)
2017-03-29 03:29:50.785916: step 6330, loss = 7.13 (151.3 examples/sec; 0.423 sec/batch)
2017-03-29 03:29:56.043348: step 6340, loss = 6.93 (148.6 examples/sec; 0.431 sec/batch)
2017-03-29 03:30:00.815328: step 6350, loss = 7.02 (141.2 examples/sec; 0.453 sec/batch)
2017-03-29 03:30:06.553276: step 6360, loss = 6.96 (145.0 examples/sec; 0.441 sec/batch)
2017-03-29 03:30:11.435996: step 6370, loss = 6.97 (140.2 examples/sec; 0.457 sec/batch)
2017-03-29 03:30:20.168781: step 6380, loss = 6.88 (149.1 examples/sec; 0.429 sec/batch)
2017-03-29 03:30:26.544789: step 6390, loss = 6.90 (149.8 examples/sec; 0.427 sec/batch)
2017-03-29 03:30:31.044448: step 6400, loss = 7.01 (143.1 examples/sec; 0.447 sec/batch)
2017-03-29 03:30:39.639843: step 6410, loss = 7.17 (142.9 examples/sec; 0.448 sec/batch)
2017-03-29 03:30:44.079286: step 6420, loss = 7.05 (142.6 examples/sec; 0.449 sec/batch)
2017-03-29 03:30:48.766307: step 6430, loss = 7.14 (135.7 examples/sec; 0.472 sec/batch)
2017-03-29 03:30:53.371720: step 6440, loss = 7.05 (135.0 examples/sec; 0.474 sec/batch)
2017-03-29 03:30:58.270788: step 6450, loss = 7.00 (138.6 examples/sec; 0.462 sec/batch)
2017-03-29 03:31:04.920059: step 6460, loss = 7.15 (147.0 examples/sec; 0.435 sec/batch)
2017-03-29 03:31:09.549953: step 6470, loss = 7.02 (146.8 examples/sec; 0.436 sec/batch)
2017-03-29 03:31:17.717640: step 6480, loss = 6.99 (129.9 examples/sec; 0.492 sec/batch)
2017-03-29 03:31:23.298137: step 6490, loss = 6.91 (140.6 examples/sec; 0.455 sec/batch)
2017-03-29 03:31:29.014930: step 6500, loss = 6.96 (142.4 examples/sec; 0.449 sec/batch)
2017-03-29 03:31:37.818633: step 6510, loss = 6.92 (146.5 examples/sec; 0.437 sec/batch)
2017-03-29 03:31:42.294133: step 6520, loss = 7.15 (128.1 examples/sec; 0.500 sec/batch)
2017-03-29 03:31:46.784191: step 6530, loss = 6.79 (130.4 examples/sec; 0.491 sec/batch)
2017-03-29 03:31:51.212264: step 6540, loss = 7.10 (140.9 examples/sec; 0.454 sec/batch)
2017-03-29 03:31:55.733952: step 6550, loss = 7.05 (144.4 examples/sec; 0.443 sec/batch)
2017-03-29 03:32:00.539935: step 6560, loss = 6.90 (138.0 examples/sec; 0.464 sec/batch)
2017-03-29 03:32:05.978995: step 6570, loss = 7.07 (152.0 examples/sec; 0.421 sec/batch)
2017-03-29 03:32:13.954027: step 6580, loss = 7.08 (137.4 examples/sec; 0.466 sec/batch)
2017-03-29 03:32:18.735297: step 6590, loss = 7.17 (147.2 examples/sec; 0.435 sec/batch)
2017-03-29 03:32:24.162873: step 6600, loss = 6.99 (68.7 examples/sec; 0.932 sec/batch)
2017-03-29 03:32:33.035309: step 6610, loss = 7.18 (140.9 examples/sec; 0.454 sec/batch)
2017-03-29 03:32:37.642741: step 6620, loss = 6.97 (139.9 examples/sec; 0.457 sec/batch)
2017-03-29 03:32:42.245820: step 6630, loss = 6.96 (144.5 examples/sec; 0.443 sec/batch)
2017-03-29 03:32:46.679377: step 6640, loss = 7.09 (149.6 examples/sec; 0.428 sec/batch)
2017-03-29 03:32:51.287559: step 6650, loss = 7.08 (143.8 examples/sec; 0.445 sec/batch)
2017-03-29 03:32:55.851263: step 6660, loss = 7.07 (153.6 examples/sec; 0.417 sec/batch)
2017-03-29 03:33:00.272156: step 6670, loss = 7.11 (151.1 examples/sec; 0.424 sec/batch)
2017-03-29 03:33:04.836182: step 6680, loss = 7.00 (147.4 examples/sec; 0.434 sec/batch)
2017-03-29 03:33:09.609462: step 6690, loss = 6.97 (128.6 examples/sec; 0.498 sec/batch)
2017-03-29 03:33:14.389605: step 6700, loss = 7.06 (146.9 examples/sec; 0.436 sec/batch)
2017-03-29 03:33:23.095643: step 6710, loss = 6.95 (152.3 examples/sec; 0.420 sec/batch)
2017-03-29 03:33:27.578936: step 6720, loss = 7.10 (126.7 examples/sec; 0.505 sec/batch)
2017-03-29 03:33:32.095861: step 6730, loss = 7.04 (145.3 examples/sec; 0.440 sec/batch)
2017-03-29 03:33:36.558080: step 6740, loss = 7.19 (140.4 examples/sec; 0.456 sec/batch)
2017-03-29 03:33:41.296041: step 6750, loss = 6.91 (151.6 examples/sec; 0.422 sec/batch)
2017-03-29 03:33:46.203982: step 6760, loss = 7.05 (137.1 examples/sec; 0.467 sec/batch)
2017-03-29 03:33:51.557336: step 6770, loss = 7.05 (138.1 examples/sec; 0.464 sec/batch)
2017-03-29 03:33:57.273863: step 6780, loss = 6.91 (138.9 examples/sec; 0.461 sec/batch)
2017-03-29 03:34:01.656325: step 6790, loss = 6.86 (146.5 examples/sec; 0.437 sec/batch)
2017-03-29 03:34:06.307064: step 6800, loss = 6.65 (150.8 examples/sec; 0.425 sec/batch)
2017-03-29 03:34:14.900377: step 6810, loss = 7.05 (149.7 examples/sec; 0.428 sec/batch)
2017-03-29 03:34:19.518153: step 6820, loss = 7.01 (148.3 examples/sec; 0.431 sec/batch)
2017-03-29 03:34:24.148221: step 6830, loss = 6.78 (142.4 examples/sec; 0.450 sec/batch)
2017-03-29 03:34:28.774277: step 6840, loss = 6.86 (145.9 examples/sec; 0.439 sec/batch)
2017-03-29 03:34:34.426299: step 6850, loss = 6.98 (122.5 examples/sec; 0.522 sec/batch)
2017-03-29 03:34:41.215989: step 6860, loss = 6.89 (128.1 examples/sec; 0.499 sec/batch)
2017-03-29 03:34:46.048067: step 6870, loss = 7.13 (146.9 examples/sec; 0.436 sec/batch)
2017-03-29 03:34:50.519470: step 6880, loss = 6.95 (142.6 examples/sec; 0.449 sec/batch)
2017-03-29 03:34:54.883271: step 6890, loss = 6.87 (145.7 examples/sec; 0.439 sec/batch)
2017-03-29 03:34:59.418121: step 6900, loss = 7.04 (134.4 examples/sec; 0.476 sec/batch)
2017-03-29 03:35:07.938958: step 6910, loss = 7.10 (129.7 examples/sec; 0.493 sec/batch)
2017-03-29 03:35:12.346684: step 6920, loss = 6.93 (152.8 examples/sec; 0.419 sec/batch)
2017-03-29 03:35:16.826006: step 6930, loss = 7.04 (153.7 examples/sec; 0.416 sec/batch)
2017-03-29 03:35:21.283730: step 6940, loss = 6.92 (146.7 examples/sec; 0.436 sec/batch)
2017-03-29 03:35:25.999568: step 6950, loss = 6.98 (143.3 examples/sec; 0.446 sec/batch)
2017-03-29 03:35:30.424366: step 6960, loss = 6.87 (134.3 examples/sec; 0.477 sec/batch)
2017-03-29 03:35:34.869145: step 6970, loss = 7.02 (137.9 examples/sec; 0.464 sec/batch)
2017-03-29 03:35:39.417388: step 6980, loss = 7.05 (149.0 examples/sec; 0.430 sec/batch)
2017-03-29 03:35:44.312534: step 6990, loss = 7.11 (147.2 examples/sec; 0.435 sec/batch)
2017-03-29 03:35:49.562484: step 7000, loss = 6.84 (142.0 examples/sec; 0.451 sec/batch)
2017-03-29 03:35:58.250850: step 7010, loss = 6.78 (152.8 examples/sec; 0.419 sec/batch)
2017-03-29 03:36:02.844155: step 7020, loss = 6.76 (134.8 examples/sec; 0.475 sec/batch)
2017-03-29 03:36:07.260767: step 7030, loss = 7.06 (143.8 examples/sec; 0.445 sec/batch)
2017-03-29 03:36:11.698256: step 7040, loss = 7.08 (153.9 examples/sec; 0.416 sec/batch)
2017-03-29 03:36:16.127022: step 7050, loss = 7.10 (146.2 examples/sec; 0.438 sec/batch)
2017-03-29 03:36:20.780553: step 7060, loss = 6.84 (141.4 examples/sec; 0.453 sec/batch)
2017-03-29 03:36:25.577018: step 7070, loss = 7.17 (130.6 examples/sec; 0.490 sec/batch)
2017-03-29 03:36:30.217632: step 7080, loss = 6.92 (132.8 examples/sec; 0.482 sec/batch)
2017-03-29 03:36:34.898068: step 7090, loss = 7.14 (147.9 examples/sec; 0.433 sec/batch)
2017-03-29 03:36:39.438178: step 7100, loss = 6.70 (145.7 examples/sec; 0.439 sec/batch)
2017-03-29 03:36:48.301849: step 7110, loss = 6.84 (150.2 examples/sec; 0.426 sec/batch)
2017-03-29 03:36:52.954114: step 7120, loss = 6.99 (140.0 examples/sec; 0.457 sec/batch)
2017-03-29 03:36:57.406103: step 7130, loss = 6.95 (148.7 examples/sec; 0.430 sec/batch)
2017-03-29 03:37:02.192630: step 7140, loss = 6.92 (143.3 examples/sec; 0.446 sec/batch)
2017-03-29 03:37:06.586033: step 7150, loss = 6.99 (142.4 examples/sec; 0.449 sec/batch)
2017-03-29 03:37:11.271701: step 7160, loss = 6.97 (136.1 examples/sec; 0.470 sec/batch)
2017-03-29 03:37:15.884477: step 7170, loss = 6.99 (141.3 examples/sec; 0.453 sec/batch)
2017-03-29 03:37:20.511127: step 7180, loss = 7.19 (144.5 examples/sec; 0.443 sec/batch)
2017-03-29 03:37:25.187838: step 7190, loss = 7.20 (137.7 examples/sec; 0.465 sec/batch)
2017-03-29 03:37:29.690174: step 7200, loss = 6.98 (133.1 examples/sec; 0.481 sec/batch)
2017-03-29 03:37:38.576814: step 7210, loss = 7.22 (136.5 examples/sec; 0.469 sec/batch)
2017-03-29 03:37:43.096069: step 7220, loss = 7.30 (130.0 examples/sec; 0.492 sec/batch)
2017-03-29 03:37:47.725523: step 7230, loss = 7.13 (151.1 examples/sec; 0.424 sec/batch)
2017-03-29 03:37:52.374601: step 7240, loss = 6.87 (137.4 examples/sec; 0.466 sec/batch)
2017-03-29 03:37:57.050985: step 7250, loss = 7.09 (141.2 examples/sec; 0.453 sec/batch)
2017-03-29 03:38:01.464682: step 7260, loss = 7.14 (136.6 examples/sec; 0.468 sec/batch)
2017-03-29 03:38:06.012826: step 7270, loss = 7.05 (135.4 examples/sec; 0.473 sec/batch)
2017-03-29 03:38:10.777652: step 7280, loss = 7.07 (147.6 examples/sec; 0.434 sec/batch)
2017-03-29 03:38:15.334941: step 7290, loss = 7.09 (146.0 examples/sec; 0.438 sec/batch)
2017-03-29 03:38:19.829182: step 7300, loss = 7.11 (148.9 examples/sec; 0.430 sec/batch)
2017-03-29 03:38:28.333953: step 7310, loss = 7.00 (138.6 examples/sec; 0.462 sec/batch)
2017-03-29 03:38:32.838083: step 7320, loss = 6.91 (139.0 examples/sec; 0.460 sec/batch)
2017-03-29 03:38:37.392802: step 7330, loss = 7.00 (156.8 examples/sec; 0.408 sec/batch)
2017-03-29 03:38:42.010343: step 7340, loss = 6.92 (136.4 examples/sec; 0.469 sec/batch)
2017-03-29 03:38:46.659597: step 7350, loss = 7.26 (150.4 examples/sec; 0.426 sec/batch)
2017-03-29 03:38:51.295931: step 7360, loss = 6.92 (126.5 examples/sec; 0.506 sec/batch)
2017-03-29 03:38:55.946883: step 7370, loss = 7.09 (144.9 examples/sec; 0.442 sec/batch)
2017-03-29 03:39:00.392352: step 7380, loss = 6.96 (151.5 examples/sec; 0.422 sec/batch)
2017-03-29 03:39:04.973744: step 7390, loss = 6.88 (138.2 examples/sec; 0.463 sec/batch)
2017-03-29 03:39:09.581119: step 7400, loss = 7.03 (142.1 examples/sec; 0.450 sec/batch)
2017-03-29 03:39:18.361210: step 7410, loss = 7.03 (152.6 examples/sec; 0.419 sec/batch)
2017-03-29 03:39:22.768486: step 7420, loss = 6.96 (148.5 examples/sec; 0.431 sec/batch)
2017-03-29 03:39:27.330075: step 7430, loss = 7.01 (144.2 examples/sec; 0.444 sec/batch)
2017-03-29 03:39:32.087672: step 7440, loss = 7.20 (123.7 examples/sec; 0.517 sec/batch)
2017-03-29 03:39:36.653851: step 7450, loss = 6.95 (141.8 examples/sec; 0.451 sec/batch)
2017-03-29 03:39:41.150734: step 7460, loss = 7.01 (140.9 examples/sec; 0.454 sec/batch)
2017-03-29 03:39:45.605480: step 7470, loss = 7.07 (144.2 examples/sec; 0.444 sec/batch)
2017-03-29 03:39:50.091976: step 7480, loss = 7.05 (149.9 examples/sec; 0.427 sec/batch)
2017-03-29 03:39:54.609945: step 7490, loss = 7.09 (138.8 examples/sec; 0.461 sec/batch)
2017-03-29 03:39:59.306663: step 7500, loss = 7.21 (120.3 examples/sec; 0.532 sec/batch)
2017-03-29 03:40:08.260521: step 7510, loss = 7.15 (149.0 examples/sec; 0.430 sec/batch)
2017-03-29 03:40:12.919971: step 7520, loss = 7.00 (129.5 examples/sec; 0.494 sec/batch)
2017-03-29 03:40:17.481449: step 7530, loss = 6.88 (154.6 examples/sec; 0.414 sec/batch)
2017-03-29 03:40:22.100873: step 7540, loss = 6.75 (143.7 examples/sec; 0.445 sec/batch)
2017-03-29 03:40:26.785361: step 7550, loss = 6.92 (129.0 examples/sec; 0.496 sec/batch)
2017-03-29 03:40:31.469163: step 7560, loss = 7.01 (137.8 examples/sec; 0.465 sec/batch)
2017-03-29 03:40:36.044350: step 7570, loss = 6.96 (136.1 examples/sec; 0.470 sec/batch)
2017-03-29 03:40:40.493090: step 7580, loss = 6.97 (148.5 examples/sec; 0.431 sec/batch)
2017-03-29 03:40:45.011950: step 7590, loss = 7.14 (139.6 examples/sec; 0.458 sec/batch)
2017-03-29 03:40:49.658383: step 7600, loss = 7.11 (131.7 examples/sec; 0.486 sec/batch)
2017-03-29 03:40:58.269568: step 7610, loss = 7.08 (135.8 examples/sec; 0.471 sec/batch)
2017-03-29 03:41:03.033873: step 7620, loss = 7.09 (137.5 examples/sec; 0.466 sec/batch)
2017-03-29 03:41:07.581617: step 7630, loss = 7.26 (146.5 examples/sec; 0.437 sec/batch)
2017-03-29 03:41:12.137471: step 7640, loss = 6.94 (136.2 examples/sec; 0.470 sec/batch)
2017-03-29 03:41:16.802225: step 7650, loss = 6.96 (131.1 examples/sec; 0.488 sec/batch)
2017-03-29 03:41:21.425223: step 7660, loss = 6.99 (139.4 examples/sec; 0.459 sec/batch)
2017-03-29 03:41:26.075284: step 7670, loss = 6.89 (130.5 examples/sec; 0.490 sec/batch)
2017-03-29 03:41:30.738248: step 7680, loss = 7.03 (142.9 examples/sec; 0.448 sec/batch)
2017-03-29 03:41:35.413819: step 7690, loss = 7.03 (142.9 examples/sec; 0.448 sec/batch)
2017-03-29 03:41:39.952493: step 7700, loss = 7.00 (143.5 examples/sec; 0.446 sec/batch)
2017-03-29 03:41:48.961525: step 7710, loss = 7.24 (152.9 examples/sec; 0.419 sec/batch)
2017-03-29 03:41:53.529491: step 7720, loss = 6.98 (142.0 examples/sec; 0.451 sec/batch)
2017-03-29 03:41:58.030315: step 7730, loss = 7.07 (146.7 examples/sec; 0.436 sec/batch)
2017-03-29 03:42:02.645386: step 7740, loss = 7.08 (139.7 examples/sec; 0.458 sec/batch)
2017-03-29 03:42:07.175300: step 7750, loss = 7.13 (141.1 examples/sec; 0.454 sec/batch)
2017-03-29 03:42:11.776761: step 7760, loss = 6.87 (132.8 examples/sec; 0.482 sec/batch)
2017-03-29 03:42:16.297332: step 7770, loss = 7.10 (151.7 examples/sec; 0.422 sec/batch)
2017-03-29 03:42:20.884591: step 7780, loss = 7.01 (133.4 examples/sec; 0.480 sec/batch)
2017-03-29 03:42:25.535415: step 7790, loss = 6.75 (130.9 examples/sec; 0.489 sec/batch)
2017-03-29 03:42:30.194985: step 7800, loss = 7.17 (132.7 examples/sec; 0.482 sec/batch)
2017-03-29 03:42:38.947030: step 7810, loss = 6.96 (124.4 examples/sec; 0.514 sec/batch)
2017-03-29 03:42:43.390181: step 7820, loss = 7.08 (142.6 examples/sec; 0.449 sec/batch)
2017-03-29 03:42:48.041897: step 7830, loss = 7.14 (127.1 examples/sec; 0.503 sec/batch)
2017-03-29 03:42:52.788221: step 7840, loss = 7.01 (128.1 examples/sec; 0.500 sec/batch)
2017-03-29 03:42:57.383326: step 7850, loss = 6.95 (148.4 examples/sec; 0.431 sec/batch)
2017-03-29 03:43:01.851295: step 7860, loss = 6.98 (140.2 examples/sec; 0.456 sec/batch)
2017-03-29 03:43:06.385775: step 7870, loss = 6.89 (139.2 examples/sec; 0.460 sec/batch)
2017-03-29 03:43:11.018916: step 7880, loss = 6.95 (153.0 examples/sec; 0.418 sec/batch)
2017-03-29 03:43:15.651779: step 7890, loss = 6.86 (133.4 examples/sec; 0.480 sec/batch)
2017-03-29 03:43:20.298877: step 7900, loss = 7.14 (130.4 examples/sec; 0.491 sec/batch)
2017-03-29 03:43:29.121134: step 7910, loss = 7.08 (130.8 examples/sec; 0.489 sec/batch)
2017-03-29 03:43:33.751784: step 7920, loss = 7.01 (134.4 examples/sec; 0.476 sec/batch)
2017-03-29 03:43:38.287592: step 7930, loss = 6.95 (144.7 examples/sec; 0.442 sec/batch)
2017-03-29 03:43:42.876775: step 7940, loss = 6.82 (128.7 examples/sec; 0.497 sec/batch)
2017-03-29 03:43:47.363327: step 7950, loss = 7.01 (138.5 examples/sec; 0.462 sec/batch)
2017-03-29 03:43:52.038282: step 7960, loss = 6.92 (140.2 examples/sec; 0.456 sec/batch)
2017-03-29 03:43:56.495401: step 7970, loss = 6.94 (151.8 examples/sec; 0.422 sec/batch)
2017-03-29 03:44:01.074865: step 7980, loss = 6.91 (140.4 examples/sec; 0.456 sec/batch)
2017-03-29 03:44:05.639088: step 7990, loss = 6.96 (144.1 examples/sec; 0.444 sec/batch)
2017-03-29 03:44:10.202267: step 8000, loss = 6.93 (133.8 examples/sec; 0.478 sec/batch)
2017-03-29 03:44:19.202309: step 8010, loss = 7.00 (140.8 examples/sec; 0.455 sec/batch)
2017-03-29 03:44:23.759781: step 8020, loss = 7.04 (150.7 examples/sec; 0.425 sec/batch)
2017-03-29 03:44:28.329680: step 8030, loss = 6.85 (138.2 examples/sec; 0.463 sec/batch)
2017-03-29 03:44:32.773493: step 8040, loss = 6.98 (138.2 examples/sec; 0.463 sec/batch)
2017-03-29 03:44:37.481972: step 8050, loss = 6.85 (140.4 examples/sec; 0.456 sec/batch)
2017-03-29 03:44:42.173303: step 8060, loss = 7.16 (130.5 examples/sec; 0.490 sec/batch)
2017-03-29 03:44:46.725699: step 8070, loss = 6.76 (142.0 examples/sec; 0.451 sec/batch)
2017-03-29 03:44:51.381516: step 8080, loss = 6.88 (140.8 examples/sec; 0.455 sec/batch)
2017-03-29 03:44:55.858774: step 8090, loss = 6.93 (142.6 examples/sec; 0.449 sec/batch)
2017-03-29 03:45:00.445922: step 8100, loss = 7.06 (137.0 examples/sec; 0.467 sec/batch)
2017-03-29 03:45:09.788825: step 8110, loss = 6.79 (147.9 examples/sec; 0.433 sec/batch)
2017-03-29 03:45:14.475547: step 8120, loss = 6.97 (132.1 examples/sec; 0.484 sec/batch)
2017-03-29 03:45:19.167099: step 8130, loss = 7.01 (142.9 examples/sec; 0.448 sec/batch)
2017-03-29 03:45:23.778443: step 8140, loss = 7.06 (140.4 examples/sec; 0.456 sec/batch)
2017-03-29 03:45:28.396276: step 8150, loss = 7.00 (153.2 examples/sec; 0.418 sec/batch)
2017-03-29 03:45:33.101076: step 8160, loss = 7.09 (142.4 examples/sec; 0.450 sec/batch)
2017-03-29 03:45:37.698747: step 8170, loss = 7.01 (131.1 examples/sec; 0.488 sec/batch)
2017-03-29 03:45:42.140952: step 8180, loss = 6.94 (146.0 examples/sec; 0.438 sec/batch)
2017-03-29 03:45:46.760563: step 8190, loss = 6.95 (136.4 examples/sec; 0.469 sec/batch)
2017-03-29 03:45:51.207068: step 8200, loss = 6.98 (143.0 examples/sec; 0.447 sec/batch)
2017-03-29 03:45:59.658490: step 8210, loss = 6.97 (143.3 examples/sec; 0.447 sec/batch)
2017-03-29 03:46:04.248399: step 8220, loss = 6.89 (142.9 examples/sec; 0.448 sec/batch)
2017-03-29 03:46:08.877949: step 8230, loss = 6.88 (130.0 examples/sec; 0.492 sec/batch)
2017-03-29 03:46:13.608078: step 8240, loss = 6.68 (123.9 examples/sec; 0.516 sec/batch)
2017-03-29 03:46:18.281406: step 8250, loss = 6.94 (135.8 examples/sec; 0.471 sec/batch)
2017-03-29 03:46:22.891706: step 8260, loss = 6.92 (147.2 examples/sec; 0.435 sec/batch)
2017-03-29 03:46:27.528463: step 8270, loss = 6.76 (134.4 examples/sec; 0.476 sec/batch)
2017-03-29 03:46:32.051184: step 8280, loss = 6.90 (149.8 examples/